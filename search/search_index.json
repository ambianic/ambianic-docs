{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An Open Source Smart Camera Project Ambianic is an award winning Open Source Smart Camera project optimized for Raspberry Pi 4B. It aims to make our homes and workspaces safer by providing helpful and actionable suggestions. For example, notify family members instantly if a loved one has fallen down. Ambianic has two major components: Ambianic Edge : an IoT device with a camera and a built-in AI engine for on-device inference and training. Usually runs within an Ambianic Box enclosure. Ambianic UI : a front-end Progressive Web App to manage Ambianic Edge devices. Following is a high level diagram of a typical Ambianic deployment. Quick Start Guide | Try it Now | View on Github |","title":"Home"},{"location":"#an-open-source-smart-camera-project","text":"Ambianic is an award winning Open Source Smart Camera project optimized for Raspberry Pi 4B. It aims to make our homes and workspaces safer by providing helpful and actionable suggestions. For example, notify family members instantly if a loved one has fallen down. Ambianic has two major components: Ambianic Edge : an IoT device with a camera and a built-in AI engine for on-device inference and training. Usually runs within an Ambianic Box enclosure. Ambianic UI : a front-end Progressive Web App to manage Ambianic Edge devices. Following is a high level diagram of a typical Ambianic deployment. Quick Start Guide | Try it Now | View on Github |","title":"An Open Source Smart Camera Project"},{"location":"developers/api-overview/","text":"API Overview REST Backend Python Frontend JS Other","title":"API Overview"},{"location":"developers/api-overview/#api-overview","text":"REST Backend Python Frontend JS Other","title":"API Overview"},{"location":"developers/api/","text":"Ambianic APIs API Overview More about our API architecture REST API More about REST API WebSockets API More about our WebSockets API MQTT API More about our MQTT API Backend Python API More about our Python API Frontend JavaScript API More about our JavaScript API","title":"Ambianic APIs"},{"location":"developers/api/#ambianic-apis","text":"","title":"Ambianic APIs"},{"location":"developers/api/#api-overview","text":"More about our API architecture","title":"API Overview"},{"location":"developers/api/#rest-api","text":"More about REST API","title":"REST API"},{"location":"developers/api/#websockets-api","text":"More about our WebSockets API","title":"WebSockets API"},{"location":"developers/api/#mqtt-api","text":"More about our MQTT API","title":"MQTT API"},{"location":"developers/api/#backend-python-api","text":"More about our Python API","title":"Backend Python API"},{"location":"developers/api/#frontend-javascript-api","text":"More about our JavaScript API","title":"Frontend JavaScript API"},{"location":"developers/architecture/","text":"Ambianic System Level Architecture Ambianic High Level Flow Diagram Ambianic User Personas There are two major types of user personas we see in our community: 1. DYI folks and developers who love the ultimate control and flexibility of an Open Source privacy preserving surveillance solution. 2. End users who love the concept of a no nonsense privacy preserving solution to help care for their household and remote family members. User group 1 (i.e. cohort) cares more about freedom to fork, hack and integrate the code any way they like with any other systems they want to. This group is about 2% of all people who have been involved in some capacity with Ambianic.ai. User group 2, doesn't have the skills, time or plain interest to get messy with integrating an AI component with an existing camera system. They would rather buy a new turn-key intelligent camera up to $200. If they had an old camera installed (presumably a $50-100 investment), they would rather return it and replace with the a new one or simply donate it to a charity. If the old camera has been used more than 6 months, then its a depreciated asset that served its purpose and can be now written off. $100 is generally a tolerable pain threshold as compared to spending hours on integration or hiring an expert to do the work, which would cost much more than a new drop-in replacement. Both user groups are important to the long term success of the project. User Group 1 is vital, because these are the kinds of folks who can thoroughly dissect the software and offer valuable constructive criticism, contribute code, docs and in the future help grow the core team. User Group 2, the remaining 98%, is obviously vital, because these are the majority of end users who we ultimately aim to provide value to. Just to be clear, Ambianic.ai is primarily focused on adding tangible value to real users today. The fact that its open source does not contradict the primary goal in any way. Open Source is meant to enforce its validity of true value by removing proprietary layers of obscurity and superficial flashy claims. As a high level roadmap decision, we are moving forward by acknowledging that both User Group 1 and 2 are important and the project should proceed in a way that doesn't lock out either one from participating. To that extend, Luca Capra (one of our new team members) summarized the way forward better than I could: I am convinced that every niche expect and deserve a specific UX. I think the core could be flexible enough to support generic functionalities while delegating domain specific challenges to a vertical application. This could happen through different layers of integrations, eventually reusable. An example could be an object detection pipeline for training and execution, adapted to home care but also to security. Same functional perimeter but with different specializations based on user requirements Ambianic Edge Device Here is a high level architecture diagram of an Ambianic Edge device. ) And here is a composition diagram showing APIs and layers of each Ambianic Edge component. Ambianic Edge Components Pipelines Pipe Elements Sources Outputs Connecting Pipelines Integrations Ambianic's main goal is to observe the environment and provide helpful suggestions in the context of home and business automation. The user has complete ownership and control of Ambianic's input sensors, AI inference flows and processed data storage. All logic executes on user's own edge devices and data is stored only on these devices with optional backup on a user designated server. The main unit of work is the Ambianic pipeline which executes in the runtime of Ambianic Edge devices. The following diagram illustrates an example pipeline which takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart(); Configuration Here is the corresponding configuration section for the pipeline above (normally located in config.yaml ): pipelines: daytime_front_door_watch: - source: rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101 - detect_objects: # run ai inference on the input data model: tflite: ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: ai_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite labels: ai_models/coco_labels.txt confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: ./data/detections/front-door/objects positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output model: tflite: ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: ai_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite labels: ai_models/coco_labels.txt confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: ./data/detections/front-door/faces positive_interval: 2 idle_interval: 600 Architectural Decomposition The following slides provide an alternative perspective on the Ambianic Edge architecture.","title":"High Level System Architecture"},{"location":"developers/architecture/#ambianic-system-level-architecture","text":"","title":"Ambianic System Level Architecture"},{"location":"developers/architecture/#ambianic-high-level-flow-diagram","text":"","title":"Ambianic High Level Flow Diagram"},{"location":"developers/architecture/#ambianic-user-personas","text":"There are two major types of user personas we see in our community: 1. DYI folks and developers who love the ultimate control and flexibility of an Open Source privacy preserving surveillance solution. 2. End users who love the concept of a no nonsense privacy preserving solution to help care for their household and remote family members. User group 1 (i.e. cohort) cares more about freedom to fork, hack and integrate the code any way they like with any other systems they want to. This group is about 2% of all people who have been involved in some capacity with Ambianic.ai. User group 2, doesn't have the skills, time or plain interest to get messy with integrating an AI component with an existing camera system. They would rather buy a new turn-key intelligent camera up to $200. If they had an old camera installed (presumably a $50-100 investment), they would rather return it and replace with the a new one or simply donate it to a charity. If the old camera has been used more than 6 months, then its a depreciated asset that served its purpose and can be now written off. $100 is generally a tolerable pain threshold as compared to spending hours on integration or hiring an expert to do the work, which would cost much more than a new drop-in replacement. Both user groups are important to the long term success of the project. User Group 1 is vital, because these are the kinds of folks who can thoroughly dissect the software and offer valuable constructive criticism, contribute code, docs and in the future help grow the core team. User Group 2, the remaining 98%, is obviously vital, because these are the majority of end users who we ultimately aim to provide value to. Just to be clear, Ambianic.ai is primarily focused on adding tangible value to real users today. The fact that its open source does not contradict the primary goal in any way. Open Source is meant to enforce its validity of true value by removing proprietary layers of obscurity and superficial flashy claims. As a high level roadmap decision, we are moving forward by acknowledging that both User Group 1 and 2 are important and the project should proceed in a way that doesn't lock out either one from participating. To that extend, Luca Capra (one of our new team members) summarized the way forward better than I could: I am convinced that every niche expect and deserve a specific UX. I think the core could be flexible enough to support generic functionalities while delegating domain specific challenges to a vertical application. This could happen through different layers of integrations, eventually reusable. An example could be an object detection pipeline for training and execution, adapted to home care but also to security. Same functional perimeter but with different specializations based on user requirements","title":"Ambianic User Personas"},{"location":"developers/architecture/#ambianic-edge-device","text":"Here is a high level architecture diagram of an Ambianic Edge device. ) And here is a composition diagram showing APIs and layers of each Ambianic Edge component.","title":"Ambianic Edge Device"},{"location":"developers/architecture/#ambianic-edge-components","text":"Pipelines Pipe Elements Sources Outputs Connecting Pipelines Integrations Ambianic's main goal is to observe the environment and provide helpful suggestions in the context of home and business automation. The user has complete ownership and control of Ambianic's input sensors, AI inference flows and processed data storage. All logic executes on user's own edge devices and data is stored only on these devices with optional backup on a user designated server. The main unit of work is the Ambianic pipeline which executes in the runtime of Ambianic Edge devices. The following diagram illustrates an example pipeline which takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart();","title":"Ambianic Edge Components"},{"location":"developers/architecture/#configuration","text":"Here is the corresponding configuration section for the pipeline above (normally located in config.yaml ): pipelines: daytime_front_door_watch: - source: rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101 - detect_objects: # run ai inference on the input data model: tflite: ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: ai_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite labels: ai_models/coco_labels.txt confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: ./data/detections/front-door/objects positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output model: tflite: ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: ai_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite labels: ai_models/coco_labels.txt confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: ./data/detections/front-door/faces positive_interval: 2 idle_interval: 600","title":"Configuration"},{"location":"developers/architecture/#architectural-decomposition","text":"The following slides provide an alternative perspective on the Ambianic Edge architecture.","title":"Architectural Decomposition"},{"location":"developers/cloud-deployment/","text":"Cloud Deployment Scenarios Private Cloud Public Cloud","title":"Cloud Deployment Scenarios"},{"location":"developers/cloud-deployment/#cloud-deployment-scenarios","text":"Private Cloud Public Cloud","title":"Cloud Deployment Scenarios"},{"location":"developers/deployment/","text":"Deployment Scenarios Minimal deployment Reference Architecture Scaling deployments Scaling Up Scaling Out Reference Architecture","title":"Deployment Scenarios"},{"location":"developers/deployment/#deployment-scenarios","text":"Minimal deployment Reference Architecture Scaling deployments Scaling Up Scaling Out Reference Architecture","title":"Deployment Scenarios"},{"location":"developers/development-environment/","text":"Development Environment Setup This page decribes how to setup a development environment for Ambianic.ai repos. If you plan to push your own code you should fork the original github repository and work from your local copy/fork. Gitpod Continuous Development Environment The easiest way to start a development environment for Ambianic Edge and Ambianic UI is to use the Gitpod Continuous Development Environment. Gitpod is an All-in-browser Continuous Development Environment with user experience similar to VS Code, but running in a virtual cloud container instead of your local machine. We maintain a Gitpod configuration preset for Ambianic Edge and Ambianic UI as part of the upstream git repository, which makes it as easy as a single click to launch a fully configured and ready to use dev environment. Gitpod offers Free No Limits Professional Open Source accounts to qualifying projects maintainers. If you have not used Gitpod before, go ahead and give it a try . You can launch your gitpod space for your forked repo in one of two ways: Point your browser to an URL composed of the gitpod web app prefix and your forked repo URL. For example: https://gitpod.io/#https://github.com/MY_GITHUB_ACCOUNT/ambianic-edge Alternatively, using the Gitpod browser extension , just hit the green gitpod button on your fork's main page: When your gitpod space starts for your repo, in the background it will install all the required dependencies, pre-commit hooks and other environment settings required to build, run and test the project. You can focus on writing, testing and committing your new code. You can study the gitpod config file ( .gitpod.yml ) located at the root of your repo for details on the virtual dev environment setup steps. Once your gitpod space starts it will automatically open and run the main modules for the project. You will also see a terminal running the full testsuite to ensure that you environment is in good shape and ready for coding. Ambianic Edge with Gitpod Once your gitpod space for your Ambianic Edge fork is launched, you will see several shell terminals automatically open and run the main modules that Ambinic Edge is composed of: 1. Core sensor monitoring and inference engine. 2. OpenAPI server (fastapi/uvicorn) 3. Peer-to-peer proxy (HTTP over WebRTC) You will also see a terminal running the full testsuite to ensure that you environment is in good shape and ready for coding. Screenshots examples for each terminal window follow: Ambianic Edge gitpod space automatically runs the full testsuite in a terminal window. Gitpod terminal running the Ambianic Edge Core monitoring and inference engine Gitpod terminal running fastapi/uvicorn serving the Ambianic Edge REST API (OpenAPI compliant) Gitpod terminal running WebRTC-HTTP Proxy which allows Ambianic UI to connect to this Ambianic Edge instance Ambianic Edge Dev Configuration The Ambainic Edge instance running in your gitpod space is configured by the dev-config.yaml file located in the dev directory of your workspace. If you want to modify the dev config and test with custom settings, you should add dev-config.yaml to your .gitignore file to ensure that you will not commit it back to the upstream repo. Notice that the default dev-config.yaml file is setup to use a local video file as input source to the AI pipelines. recorded_cam_feed: # location of the recorded video file in a gitpod (or local) dev environment uri: file:///workspace/ambianic-edge/tests/pipeline/avsource/test2-cam-person1.mkv type: video # live: true when we want the pipeline to keep looping over the same video file. # live: false when we want the pipeline to go through the video file once and stop. live: false This configuration allows you to simulate a realistic Ambianic Edge deployment without connecting to a live camera feed. You can change the source to use video file recordings of your choice. Unit Testing Ambianic Edge We use pytest to drive the Ambianic Edge testsuite. To run the full testsuite, you can go to the terminal window that runs it on launch and manually repeate the command: python3 -m pytest -v --log-cli-level=DEBUG --cov=ambianic --cov-report=term tests/ This runs all tests and prints a coverage report in the terminal. Also shows in the terminal all log messages logged by the tested code. You can use the pytest \"-k\" option to run selected tests. See the official pytest docs for other ways to run selected tests. Ambianic UI with Gitpod Once your gitpod space for your Ambianic UI fork is launched, you will see several shell terminals open and run: Ambinic PWA nodejs service. Gitpod will prompt you to open a browser tab for the PWA. Browser-sync service which allows you to open in a browser window static files from the project directories. You will also see another shell terminal running the full testsuite to ensure that you environment is in good shape and ready for coding. Screenshots examples for each terminal window follow: Ambianic UI gitpod space automatically runs the full testsuite in a terminal window. Terminal window running the Ambianic PWA as a nodejs service in dev mode. Terminal window running browser-sync. Unit testing Ambianic UI We use Jest for unit testing and Cypress for e2e testing. To run tests, go to the terminal that runs all tests when the gitpod space launches. Then you can Run all tests with npm run test Run unit tests with npm run test:unit Run e2e tests with npm run test:e2e Unit tests are located under tests/unit E2E tests are located under cypress/integration/ambianic-tests End to End System testing If you'd like to manually test UI and Edge together as a whole system, while running in dev mode in conditions as close as possible to a real world deployment, take a look at this guide . Local Development Environment Setting up a local dev environment tends to be more work as compared to Gitpod as it takes away compute resources from your workstation and it requires maintaining multiple virtual environments and containers along with developent tools for different language platforms. As the project evolves, it usually requires some local upkeep to make sure all local tools are up to date and in sync with the latest versions used for the current project build. If this is your preferred method, the following guidleninces are for you. Setting Up Ambianic Edge on a Local Worsktation To setup the dev environment for Ambianic Edge on your local machine you need as prerequisites: a *nix machine also in VM docker git Clone or fork the repository at https://github.com/ambianic/ambianic-edge Now open a terminal and move to the ambianic-edge folder. Start the development environment To start the development container run ./ambianic-debug.sh This command will download the most up to date version of the base container and mount your local directory into it. Once the setup is completed, you will have a shell prompt from inside the container. You should see something like that $ ./ambianic-debug.sh +++ dirname ./ambianic-debug.sh ++ cd . ++ pwd ++ basename ./ambianic-debug.sh + MY_PATH=/home/user/ambianic-edge/ambianic-debug.sh ++ dirname /home/user/ambianic-edge/ambianic-debug.sh + MY_DIR=/home/user/ambianic-edge + USB_DIR=/dev/bus/usb + VIDEO_0=/dev/video0 + '[' -d /dev/bus/usb ']' + USB_ARG='--device /dev/bus/usb' + VC_SHARED_LIB= + grep -s -q 'Raspberry Pi' /proc/cpuinfo + '[' -e /dev/video0 ']' + VIDEO_ARG='--device /dev/video0' + docker pull ambianic/ambianic-edge:dev dev: Pulling from ambianic/ambianic-edge Digest: sha256:1b95614012f7a12abd5604471d81ec2b6dbda6d4aa1a622e69ef1605f9ff6004 Status: Image is up to date for ambianic/ambianic-edge:dev docker.io/ambianic/ambianic-edge:dev + docker run -it --rm --name ambianic-edge-dev --mount type=bind,source=/home/user/ambianic-edge,target=/workspace --privileged --net=host --device /dev/video0 --device /dev/bus/usb ambianic/ambianic-edge:dev The shell will show up root@pc:/opt/ambianic-edge# Move to /workspace directory where the local code is mounted with cd /workspace/ Starting a development instance Run ./src/run-dev.sh to start the development instance of Ambianic.ai from the container. Running tests To run the test use ./tests/run-tests.sh The command will install the required dependencies and scan the ./tests folder to execute all the tests found. When developing is common to narrow the scope of testing to the componentes we are working on. We can use the pytest commmand directly for that purpose, follows a list of handy command that can be used during development. Note ensure to run ./tests/run-tests.sh at least once to have the python dependencies installed! # run all the tests python3 -m pytest tests/ # run only tests found from a specific file python3 -m pytest tests/pipeline/ai/test_fall_detect.py # run test by name (will match the function name) python3 -m pytest tests/ -k test_bad_sample_good_sample # run test by name match (will match the function name, also patially) python3 -m pytest tests/ -k test_bad_sample_good_sample # with just a part of it python3 -m pytest tests/ -k bad_sample_good # control CLI log level python3 -m pytest tests/ --log-cli-level DEBUG Debugging with an IDE This section contains suggestions to setup a proper development environment for better productivity. Feel free to share yours! MS VS Code vscode offers a plugin called Remote - Containers that allow to connect to a container running on a local (or remote) machine. This is particularly handy when one needs to use the debugger to inspect the runtime. It can also be used to develop on the raspberry PI from your own PC. Setting Up Ambianic Edge on a Local worsktation Setting Up Ambianic UI on a Local Workstation To build, test and debug Ambianic UI on a local laptop or desktop, you will need node.js and git installed. Clone repository git clone https://github.com/ambianic/ambianic-ui.git cd ambianic-ui Install dependencies npm install Prepare dev environment Set up a git pre-commit hook for linting. npm run prepare Compiles and hot-reloads for development npm run serve Compiles and minifies for production npm run build Runs all tests npm run test","title":"Development Environment Setup"},{"location":"developers/development-environment/#development-environment-setup","text":"This page decribes how to setup a development environment for Ambianic.ai repos. If you plan to push your own code you should fork the original github repository and work from your local copy/fork.","title":"Development Environment Setup"},{"location":"developers/development-environment/#gitpod-continuous-development-environment","text":"The easiest way to start a development environment for Ambianic Edge and Ambianic UI is to use the Gitpod Continuous Development Environment. Gitpod is an All-in-browser Continuous Development Environment with user experience similar to VS Code, but running in a virtual cloud container instead of your local machine. We maintain a Gitpod configuration preset for Ambianic Edge and Ambianic UI as part of the upstream git repository, which makes it as easy as a single click to launch a fully configured and ready to use dev environment. Gitpod offers Free No Limits Professional Open Source accounts to qualifying projects maintainers. If you have not used Gitpod before, go ahead and give it a try . You can launch your gitpod space for your forked repo in one of two ways: Point your browser to an URL composed of the gitpod web app prefix and your forked repo URL. For example: https://gitpod.io/#https://github.com/MY_GITHUB_ACCOUNT/ambianic-edge Alternatively, using the Gitpod browser extension , just hit the green gitpod button on your fork's main page: When your gitpod space starts for your repo, in the background it will install all the required dependencies, pre-commit hooks and other environment settings required to build, run and test the project. You can focus on writing, testing and committing your new code. You can study the gitpod config file ( .gitpod.yml ) located at the root of your repo for details on the virtual dev environment setup steps. Once your gitpod space starts it will automatically open and run the main modules for the project. You will also see a terminal running the full testsuite to ensure that you environment is in good shape and ready for coding.","title":"Gitpod Continuous Development Environment"},{"location":"developers/development-environment/#ambianic-edge-with-gitpod","text":"Once your gitpod space for your Ambianic Edge fork is launched, you will see several shell terminals automatically open and run the main modules that Ambinic Edge is composed of: 1. Core sensor monitoring and inference engine. 2. OpenAPI server (fastapi/uvicorn) 3. Peer-to-peer proxy (HTTP over WebRTC) You will also see a terminal running the full testsuite to ensure that you environment is in good shape and ready for coding. Screenshots examples for each terminal window follow: Ambianic Edge gitpod space automatically runs the full testsuite in a terminal window. Gitpod terminal running the Ambianic Edge Core monitoring and inference engine Gitpod terminal running fastapi/uvicorn serving the Ambianic Edge REST API (OpenAPI compliant) Gitpod terminal running WebRTC-HTTP Proxy which allows Ambianic UI to connect to this Ambianic Edge instance","title":"Ambianic Edge with Gitpod"},{"location":"developers/development-environment/#ambianic-edge-dev-configuration","text":"The Ambainic Edge instance running in your gitpod space is configured by the dev-config.yaml file located in the dev directory of your workspace. If you want to modify the dev config and test with custom settings, you should add dev-config.yaml to your .gitignore file to ensure that you will not commit it back to the upstream repo. Notice that the default dev-config.yaml file is setup to use a local video file as input source to the AI pipelines. recorded_cam_feed: # location of the recorded video file in a gitpod (or local) dev environment uri: file:///workspace/ambianic-edge/tests/pipeline/avsource/test2-cam-person1.mkv type: video # live: true when we want the pipeline to keep looping over the same video file. # live: false when we want the pipeline to go through the video file once and stop. live: false This configuration allows you to simulate a realistic Ambianic Edge deployment without connecting to a live camera feed. You can change the source to use video file recordings of your choice.","title":"Ambianic Edge Dev Configuration"},{"location":"developers/development-environment/#unit-testing-ambianic-edge","text":"We use pytest to drive the Ambianic Edge testsuite. To run the full testsuite, you can go to the terminal window that runs it on launch and manually repeate the command: python3 -m pytest -v --log-cli-level=DEBUG --cov=ambianic --cov-report=term tests/ This runs all tests and prints a coverage report in the terminal. Also shows in the terminal all log messages logged by the tested code. You can use the pytest \"-k\" option to run selected tests. See the official pytest docs for other ways to run selected tests.","title":"Unit Testing Ambianic Edge"},{"location":"developers/development-environment/#ambianic-ui-with-gitpod","text":"Once your gitpod space for your Ambianic UI fork is launched, you will see several shell terminals open and run: Ambinic PWA nodejs service. Gitpod will prompt you to open a browser tab for the PWA. Browser-sync service which allows you to open in a browser window static files from the project directories. You will also see another shell terminal running the full testsuite to ensure that you environment is in good shape and ready for coding. Screenshots examples for each terminal window follow: Ambianic UI gitpod space automatically runs the full testsuite in a terminal window. Terminal window running the Ambianic PWA as a nodejs service in dev mode. Terminal window running browser-sync.","title":"Ambianic UI with Gitpod"},{"location":"developers/development-environment/#unit-testing-ambianic-ui","text":"We use Jest for unit testing and Cypress for e2e testing. To run tests, go to the terminal that runs all tests when the gitpod space launches. Then you can Run all tests with npm run test Run unit tests with npm run test:unit Run e2e tests with npm run test:e2e Unit tests are located under tests/unit E2E tests are located under cypress/integration/ambianic-tests","title":"Unit testing Ambianic UI"},{"location":"developers/development-environment/#end-to-end-system-testing","text":"If you'd like to manually test UI and Edge together as a whole system, while running in dev mode in conditions as close as possible to a real world deployment, take a look at this guide .","title":"End to End System testing"},{"location":"developers/development-environment/#local-development-environment","text":"Setting up a local dev environment tends to be more work as compared to Gitpod as it takes away compute resources from your workstation and it requires maintaining multiple virtual environments and containers along with developent tools for different language platforms. As the project evolves, it usually requires some local upkeep to make sure all local tools are up to date and in sync with the latest versions used for the current project build. If this is your preferred method, the following guidleninces are for you.","title":"Local Development Environment"},{"location":"developers/development-environment/#setting-up-ambianic-edge-on-a-local-worsktation","text":"To setup the dev environment for Ambianic Edge on your local machine you need as prerequisites: a *nix machine also in VM docker git Clone or fork the repository at https://github.com/ambianic/ambianic-edge Now open a terminal and move to the ambianic-edge folder.","title":"Setting Up Ambianic Edge on a Local Worsktation"},{"location":"developers/development-environment/#start-the-development-environment","text":"To start the development container run ./ambianic-debug.sh This command will download the most up to date version of the base container and mount your local directory into it. Once the setup is completed, you will have a shell prompt from inside the container. You should see something like that $ ./ambianic-debug.sh +++ dirname ./ambianic-debug.sh ++ cd . ++ pwd ++ basename ./ambianic-debug.sh + MY_PATH=/home/user/ambianic-edge/ambianic-debug.sh ++ dirname /home/user/ambianic-edge/ambianic-debug.sh + MY_DIR=/home/user/ambianic-edge + USB_DIR=/dev/bus/usb + VIDEO_0=/dev/video0 + '[' -d /dev/bus/usb ']' + USB_ARG='--device /dev/bus/usb' + VC_SHARED_LIB= + grep -s -q 'Raspberry Pi' /proc/cpuinfo + '[' -e /dev/video0 ']' + VIDEO_ARG='--device /dev/video0' + docker pull ambianic/ambianic-edge:dev dev: Pulling from ambianic/ambianic-edge Digest: sha256:1b95614012f7a12abd5604471d81ec2b6dbda6d4aa1a622e69ef1605f9ff6004 Status: Image is up to date for ambianic/ambianic-edge:dev docker.io/ambianic/ambianic-edge:dev + docker run -it --rm --name ambianic-edge-dev --mount type=bind,source=/home/user/ambianic-edge,target=/workspace --privileged --net=host --device /dev/video0 --device /dev/bus/usb ambianic/ambianic-edge:dev The shell will show up root@pc:/opt/ambianic-edge# Move to /workspace directory where the local code is mounted with cd /workspace/","title":"Start the development environment"},{"location":"developers/development-environment/#starting-a-development-instance","text":"Run ./src/run-dev.sh to start the development instance of Ambianic.ai from the container.","title":"Starting a development instance"},{"location":"developers/development-environment/#running-tests","text":"To run the test use ./tests/run-tests.sh The command will install the required dependencies and scan the ./tests folder to execute all the tests found. When developing is common to narrow the scope of testing to the componentes we are working on. We can use the pytest commmand directly for that purpose, follows a list of handy command that can be used during development. Note ensure to run ./tests/run-tests.sh at least once to have the python dependencies installed! # run all the tests python3 -m pytest tests/ # run only tests found from a specific file python3 -m pytest tests/pipeline/ai/test_fall_detect.py # run test by name (will match the function name) python3 -m pytest tests/ -k test_bad_sample_good_sample # run test by name match (will match the function name, also patially) python3 -m pytest tests/ -k test_bad_sample_good_sample # with just a part of it python3 -m pytest tests/ -k bad_sample_good # control CLI log level python3 -m pytest tests/ --log-cli-level DEBUG","title":"Running tests"},{"location":"developers/development-environment/#debugging-with-an-ide","text":"This section contains suggestions to setup a proper development environment for better productivity. Feel free to share yours!","title":"Debugging with an IDE"},{"location":"developers/development-environment/#ms-vs-code","text":"vscode offers a plugin called Remote - Containers that allow to connect to a container running on a local (or remote) machine. This is particularly handy when one needs to use the debugger to inspect the runtime. It can also be used to develop on the raspberry PI from your own PC.","title":"MS VS Code"},{"location":"developers/development-environment/#setting-up-ambianic-edge-on-a-local-worsktation_1","text":"","title":"Setting Up Ambianic Edge on a Local worsktation"},{"location":"developers/development-environment/#setting-up-ambianic-ui-on-a-local-workstation","text":"To build, test and debug Ambianic UI on a local laptop or desktop, you will need node.js and git installed.","title":"Setting Up Ambianic UI on a Local Workstation"},{"location":"developers/development-environment/#clone-repository","text":"git clone https://github.com/ambianic/ambianic-ui.git cd ambianic-ui","title":"Clone repository"},{"location":"developers/development-environment/#install-dependencies","text":"npm install","title":"Install dependencies"},{"location":"developers/development-environment/#prepare-dev-environment","text":"Set up a git pre-commit hook for linting. npm run prepare","title":"Prepare dev environment"},{"location":"developers/development-environment/#compiles-and-hot-reloads-for-development","text":"npm run serve","title":"Compiles and hot-reloads for development"},{"location":"developers/development-environment/#compiles-and-minifies-for-production","text":"npm run build","title":"Compiles and minifies for production"},{"location":"developers/development-environment/#runs-all-tests","text":"npm run test","title":"Runs all tests"},{"location":"developers/e2e-manual-testing/","text":"End to End System Testing in Dev Mode In addition to writing automated unit tests for each project module and integrated tests across modules, we occasionally need to manually walk through the system to experience it the same way as an end user would. Manual testing sometimes leads to discovering new issues that can be then covered with new automated tests. Following is the recommended way to manually test an Ambianic system end to end in development mode, but as close as possible to realistic deployment conditions. First, start the Ambianic UI and Ambianic Edge gitpod environments. If you haven't done this before, then follow these instructions . Next, go to the Ambianic Edge dev environment and make sure that all main modules are running: 1. Core Monitoring and Inference Engine 1. REST API server (fastapi) 1. WebRTC - HTTP proxy In your main workspace directory you should see a file named .peerjsrc with a contents similar to this: {\"peerId\": \"77d4b8b4-bc2a-451a-9bed-28f07c3e1abc\"} Copy to the system clipboard the peerId value from this json formatted file. Just the 77d4b8b4-bc2a-451a-9bed-28f07c3e1abc part. Notice that this value is a randomly generated UUID4 which will be unique for your environment. Next, go to the Ambianic UI environment, and make sure that the Ambianic PWA is being served by nodejs. Your terminal window should look like the screenshot below. Open a browser window pointing to port 8080 of the gitpod environment serving the Ambianic PWA on nodejs in dev mode. Click on Begin Setup then on the next page click Skip in the upper right corner of the app. On the next page, select the Main menu drop down in the Nav Bar and click Settings. Then scroll down to the section titled \"Pair with remote Ambianic Edge device\". Paste the peerjs key value that you copied from the Ambianic Edge CDE. Click on Pair Remotely. In a few moments you will see snackbar status messages indicating that the UI is connecting to Ambianic Edge followed by a confirmation that connection is established. If you now scroll up to the top of the Settings page you will see the Peer ID and Version of the Ambianic Edge device you are connected to. This is in fact a connection to the gitpod CDE running Ambianic Edge in dev mode. Notice that we did not have to open any public HTTP/S ports on the Ambianic Edge CDE in order for the UI to access the virtual device. This is exactly how the UI and Edge connect in a real world deployment. Instead of relying on access to public HTTP/S endpoints, the UI and Edge connect securely and directly via WebRTC DataChannel (using peerfetch - an HTTP over WebRTC proxy). If you now navigate to the Timeline page, you will see a timeline view. The source for this view is populated from the video file configured in dev-config.yaml in the Ambianic Edge CDE. You can change the file with your own recorded video file for testing various detection scenarios. Notice how the peerfetch terminal shows a log of HTTP fetch requests and responses from the Ambianic UI instance. This is an indication that the UI and Edge are communicating. If you switch to the Fastapi terminal it will also show REST API requests served to the peerfetch proxy on behalf of the UI app (PWA). Now you have a fully setup live and realistic system that runs both Ambianic UI and Ambianic Edge in dev mode. This setup represents as closely as possible the real world experience of an end user, while allowing you, dear developer, to test, experiment and commit code.","title":"End to End System Testing in Dev Mode"},{"location":"developers/e2e-manual-testing/#end-to-end-system-testing-in-dev-mode","text":"In addition to writing automated unit tests for each project module and integrated tests across modules, we occasionally need to manually walk through the system to experience it the same way as an end user would. Manual testing sometimes leads to discovering new issues that can be then covered with new automated tests. Following is the recommended way to manually test an Ambianic system end to end in development mode, but as close as possible to realistic deployment conditions. First, start the Ambianic UI and Ambianic Edge gitpod environments. If you haven't done this before, then follow these instructions . Next, go to the Ambianic Edge dev environment and make sure that all main modules are running: 1. Core Monitoring and Inference Engine 1. REST API server (fastapi) 1. WebRTC - HTTP proxy In your main workspace directory you should see a file named .peerjsrc with a contents similar to this: {\"peerId\": \"77d4b8b4-bc2a-451a-9bed-28f07c3e1abc\"} Copy to the system clipboard the peerId value from this json formatted file. Just the 77d4b8b4-bc2a-451a-9bed-28f07c3e1abc part. Notice that this value is a randomly generated UUID4 which will be unique for your environment. Next, go to the Ambianic UI environment, and make sure that the Ambianic PWA is being served by nodejs. Your terminal window should look like the screenshot below. Open a browser window pointing to port 8080 of the gitpod environment serving the Ambianic PWA on nodejs in dev mode. Click on Begin Setup then on the next page click Skip in the upper right corner of the app. On the next page, select the Main menu drop down in the Nav Bar and click Settings. Then scroll down to the section titled \"Pair with remote Ambianic Edge device\". Paste the peerjs key value that you copied from the Ambianic Edge CDE. Click on Pair Remotely. In a few moments you will see snackbar status messages indicating that the UI is connecting to Ambianic Edge followed by a confirmation that connection is established. If you now scroll up to the top of the Settings page you will see the Peer ID and Version of the Ambianic Edge device you are connected to. This is in fact a connection to the gitpod CDE running Ambianic Edge in dev mode. Notice that we did not have to open any public HTTP/S ports on the Ambianic Edge CDE in order for the UI to access the virtual device. This is exactly how the UI and Edge connect in a real world deployment. Instead of relying on access to public HTTP/S endpoints, the UI and Edge connect securely and directly via WebRTC DataChannel (using peerfetch - an HTTP over WebRTC proxy). If you now navigate to the Timeline page, you will see a timeline view. The source for this view is populated from the video file configured in dev-config.yaml in the Ambianic Edge CDE. You can change the file with your own recorded video file for testing various detection scenarios. Notice how the peerfetch terminal shows a log of HTTP fetch requests and responses from the Ambianic UI instance. This is an indication that the UI and Edge are communicating. If you switch to the Fastapi terminal it will also show REST API requests served to the peerfetch proxy on behalf of the UI app (PWA). Now you have a fully setup live and realistic system that runs both Ambianic UI and Ambianic Edge in dev mode. This setup represents as closely as possible the real world experience of an end user, while allowing you, dear developer, to test, experiment and commit code.","title":"End to End System Testing in Dev Mode"},{"location":"developers/edge-deployment/","text":"Edge IoT Deployment Scenarios Raspberry Pi Minimal install, capacity planning, scaling, monitoring.","title":"Edge IoT Deployment Scenarios"},{"location":"developers/edge-deployment/#edge-iot-deployment-scenarios","text":"Raspberry Pi Minimal install, capacity planning, scaling, monitoring.","title":"Edge IoT Deployment Scenarios"},{"location":"developers/home-bridge-integration/","text":"Home Bridge Integration TBD","title":"Home Bridge Integration"},{"location":"developers/home-bridge-integration/#home-bridge-integration","text":"TBD","title":"Home Bridge Integration"},{"location":"developers/integration/","text":"Integration with Smart Home hubs Home Assistant More on Home Assistant Integration Home Bridge More on Home Bridge Integration","title":"Integration with Smart Home hubs"},{"location":"developers/integration/#integration-with-smart-home-hubs","text":"","title":"Integration with Smart Home hubs"},{"location":"developers/integration/#home-assistant","text":"More on Home Assistant Integration","title":"Home Assistant"},{"location":"developers/integration/#home-bridge","text":"More on Home Bridge Integration","title":"Home Bridge"},{"location":"developers/js-api/","text":"Frontend JavaScript API TBD","title":"Frontend JavaScript API"},{"location":"developers/js-api/#frontend-javascript-api","text":"TBD","title":"Frontend JavaScript API"},{"location":"developers/mqtt-api/","text":"MQTT API TBD","title":"MQTT API"},{"location":"developers/mqtt-api/#mqtt-api","text":"TBD","title":"MQTT API"},{"location":"developers/publish-bazaar/","text":"Publishing to AI Apps Bazaar TBD","title":"Publishing to AI Apps Bazaar"},{"location":"developers/publish-bazaar/#publishing-to-ai-apps-bazaar","text":"TBD","title":"Publishing to AI Apps Bazaar"},{"location":"developers/python-api/","text":"Backend Python API TBD High level description link to raw API","title":"Backend Python API"},{"location":"developers/python-api/#backend-python-api","text":"TBD High level description link to raw API","title":"Backend Python API"},{"location":"developers/rest-api/","text":"REST API TBD","title":"REST API"},{"location":"developers/rest-api/#rest-api","text":"TBD","title":"REST API"},{"location":"developers/websockets-api/","text":"WebSockets API TBD","title":"WebSockets API"},{"location":"developers/websockets-api/#websockets-api","text":"TBD","title":"WebSockets API"},{"location":"developers/writing-plugins-overview/","text":"How to Write and Publish an Ambianic Plugin Ambianic can be extended via custom plugins. Two types of plugins are supported: Edge plugins, and UI plugins Edge plugins execute in an Ambianic Edge device environment, whereas UI plugins execute in the Ambianic UI PWA environment. It is common for a plugin to be built as a package with an Edge and an UI plugin designed to work together for a smooth end-to-end user experience. Let's take for example a VideoSource plugin that reads data from video streams such as live security camera feeds and feeds image frames to an Ambianic AI flow for inference such as person detection. The plugin will have two parts: An edge device plugin VideoSourceEdge which will reside in an Ambianic Edge device and execute in the core Ambianic runtime. It will be installed and configured on the edge device via secure shell access, its public REST APIs or GUI exposed by the VideoSourceUI plugin. An UI plugin VideoSourceUI which will implement GUI widgets for users to configure and control VideoSource connections. This plugin will enrich the visual user experience with the Ambianic UI PWA. st=>start: VideoSource Plugin install=>parallel: Install Plugin edge_plugin=>operation: VideoSourceEdge edge_env=>operation: Ambianic Edge Device src_connect=>inputoutput: Process video streams edge_flow=>operation: AI flow ui_plugin=>operation: VideoSourceUI ui_env=>operation: Ambianic UI PWA user_config=>inputoutput: GUI Configuration st(bottom)->install install(path1, right)->edge_plugin edge_plugin(bottom)->edge_env edge_env(bottom)->src_connect src_connect(bottom)->edge_flow install(path2, bottom)->ui_plugin ui_plugin(bottom)->ui_env ui_env(bottom)->user_config $(\".plugin-overview-diagram\").flowchart(); Anatomy of an Ambianic Edge plugin Ambianic Edge plugins are written in Python. They are essentially pipeline elements. See the architecture document for an overview of pipelines and pipeline elements. An edge plugin must provide at a minimum: Input: Declare input data types Provide implementation of input data consumer Output: Declare output data types Provide implementation of output data producer Configuration: Declare configuration parameters Lifecycle methods: Start Stop Heal An edge plugin may also provide: REST API: Declare public REST APIs in OpenAPI format. Provide implementation for REST APIs Anatomy of an Ambianic UI plugin An UI plugin must provide at a minimum: Install/Uninstall visual UI flow Configuration visual UI flow Runtime data views: Event Timeline: inputs and outputs Alerts: Critical, Warning, Info Stats: such as performance and error rates Event curation: Prioritization: Alert Warning Info Avatar: Image Name Color Follow up Actions: Checkbox Button Link Text Speech Gesture Custom Data curation for plugins with AI inference: Infenrece label correction Adding new labels Removing labels Plugin bundles A plugin bundle consists of: Edge plugin UI plugin A basic edge plugin template The following abstract Python class implements the minimum sceleton of an edge plugin. Plugin implementations should extend this class and add custom logic as we will see further in this document with a more advanced example. class PipeElement(ManagedService): \"\"\"The basic building block of an Ambianic pipeline.\"\"\" def __init__(self): super().__init__() self._state = PIPE_STATE_STOPPED self._next_element = None self._latest_heartbeat = time.monotonic() @property def state(self): \"\"\"Lifecycle state of the pipe element.\"\"\" return self._state def start(self): \"\"\"Only sourcing elements (first in a pipeline) need to override. It is invoked once when the enclosing pipeline is started. It should continue to run until the corresponding stop() method is invoked on the same object from a separate pipeline lifecycle manager thread. It is recommended for overriding methods to invoke this base method via super().start() before proceeding with custom logic. \"\"\" self._state = PIPE_STATE_RUNNING @abc.abstractmethod def heal(self): # pragma: no cover \"\"\"Override with adequate implementation of a healing procedure. heal() is invoked by a lifecycle manager when its determined that the element does not respond within reasonable timeframe. This can happen for example if the element depends on external IO resources, which become unavailable for an extended period of time. The healing procedure should be considered a chance to recover or find an alternative way to proceed. If heal does not reset the pipe element back to a responsive state, it is likely that the lifecycle manager will stop the element and its ecnlosing pipeline. \"\"\" pass def healthcheck(self): \"\"\"Check the health of this element. :returns: (timestamp, status) tuple with most recent heartbeat timestamp and health status code ('OK' normally). \"\"\" status = 'OK' # At some point status may carry richer information return self._latest_heartbeat, status def heartbeat(self): \"\"\"Set the heartbeat timestamp to time.monotonic(). Keeping the heartbeat timestamp current informs the lifecycle manager that this element is functioning well. \"\"\" now = time.monotonic() self._latest_heartbeat = now def stop(self): \"\"\"Receive stop signal and act accordingly. Subclasses implementing sourcing elements should override this method by first invoking their super class implementation and then running through steps specific to stopping their ongoing sample processing. \"\"\" self._state = PIPE_STATE_STOPPED def connect_to_next_element(self, next_element=None): \"\"\"Connect this element to the next element in the pipe. Subclasses should not override this method. \"\"\" assert next_element assert isinstance(next_element, PipeElement) self._next_element = next_element def receive_next_sample(self, **sample): \"\"\"Receive next sample from a connected previous element if applicable. All pipeline elements except for the first (sourcing) element in the pipeline will depend on this method to feed them with new samples to process. Subclasses should not override this method. :Parameters: ---------- **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. \"\"\" self.heartbeat() for processed_sample in self.process_sample(**sample): if self._next_element: if (processed_sample): self._next_element.receive_next_sample(**processed_sample) else: self._next_element.receive_next_sample() self.heartbeat() @abc.abstractmethod # pragma: no cover def process_sample(self, **sample) -> Iterable[dict]: \"\"\"Override and implement as generator. Invoked by receive_next_sample() when the previous element (or pipeline source) feeds another data input sample. Implementing subclasses should process input samples and yield output samples for the next element in the pipeline. :Parameters: ---------- **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. :Returns: ---------- processed_sample: Iterable[dict] Generates processed samples to be passed on to the next pipeline element. \"\"\" yield sample The following sequence diagram illustrates an example flow of data from an external security camera source through VideoSourceEdge and the rest of an object detection pipeline. sequenceDiagram participant Cam as Security Camera participant Source as VideoSourceEdge participant Detect as ObjectDetection participant Store as StoreDetection participant File as File System loop Until pipeline.stop() Cam->>Source: rtsp://ip/video/channel0 Source->>Detect: process_sample(image) Detect->>Store: process_sample(det_boxes) Store->>File: file.write(image, det_json) end A basic UI plugin template A basic plugin bundle template A more advanced plugin example Required components of a plugin Optional components of a plugin Writing a custom plugins: step by step Testing plugins Packaging plugins Packaging and Edge plugin Use a standard python wheel and publish on Pypi Packaging and UI plugin Use a standard npm package format and publish on npm Packaging a bundle of plugins In the UI plugin, provide reference to the edge plugin package: PyPi wheel name package version Documenting plugins Publishing plugins More about Publishing your AI app","title":"How to Write and Publish an Ambianic Plugin"},{"location":"developers/writing-plugins-overview/#how-to-write-and-publish-an-ambianic-plugin","text":"Ambianic can be extended via custom plugins. Two types of plugins are supported: Edge plugins, and UI plugins Edge plugins execute in an Ambianic Edge device environment, whereas UI plugins execute in the Ambianic UI PWA environment. It is common for a plugin to be built as a package with an Edge and an UI plugin designed to work together for a smooth end-to-end user experience. Let's take for example a VideoSource plugin that reads data from video streams such as live security camera feeds and feeds image frames to an Ambianic AI flow for inference such as person detection. The plugin will have two parts: An edge device plugin VideoSourceEdge which will reside in an Ambianic Edge device and execute in the core Ambianic runtime. It will be installed and configured on the edge device via secure shell access, its public REST APIs or GUI exposed by the VideoSourceUI plugin. An UI plugin VideoSourceUI which will implement GUI widgets for users to configure and control VideoSource connections. This plugin will enrich the visual user experience with the Ambianic UI PWA. st=>start: VideoSource Plugin install=>parallel: Install Plugin edge_plugin=>operation: VideoSourceEdge edge_env=>operation: Ambianic Edge Device src_connect=>inputoutput: Process video streams edge_flow=>operation: AI flow ui_plugin=>operation: VideoSourceUI ui_env=>operation: Ambianic UI PWA user_config=>inputoutput: GUI Configuration st(bottom)->install install(path1, right)->edge_plugin edge_plugin(bottom)->edge_env edge_env(bottom)->src_connect src_connect(bottom)->edge_flow install(path2, bottom)->ui_plugin ui_plugin(bottom)->ui_env ui_env(bottom)->user_config $(\".plugin-overview-diagram\").flowchart();","title":"How to Write and Publish an Ambianic Plugin"},{"location":"developers/writing-plugins-overview/#anatomy-of-an-ambianic-edge-plugin","text":"Ambianic Edge plugins are written in Python. They are essentially pipeline elements. See the architecture document for an overview of pipelines and pipeline elements. An edge plugin must provide at a minimum: Input: Declare input data types Provide implementation of input data consumer Output: Declare output data types Provide implementation of output data producer Configuration: Declare configuration parameters Lifecycle methods: Start Stop Heal An edge plugin may also provide: REST API: Declare public REST APIs in OpenAPI format. Provide implementation for REST APIs","title":"Anatomy of an Ambianic Edge plugin"},{"location":"developers/writing-plugins-overview/#anatomy-of-an-ambianic-ui-plugin","text":"An UI plugin must provide at a minimum: Install/Uninstall visual UI flow Configuration visual UI flow Runtime data views: Event Timeline: inputs and outputs Alerts: Critical, Warning, Info Stats: such as performance and error rates Event curation: Prioritization: Alert Warning Info Avatar: Image Name Color Follow up Actions: Checkbox Button Link Text Speech Gesture Custom Data curation for plugins with AI inference: Infenrece label correction Adding new labels Removing labels","title":"Anatomy of an Ambianic UI plugin"},{"location":"developers/writing-plugins-overview/#plugin-bundles","text":"A plugin bundle consists of: Edge plugin UI plugin","title":"Plugin bundles"},{"location":"developers/writing-plugins-overview/#a-basic-edge-plugin-template","text":"The following abstract Python class implements the minimum sceleton of an edge plugin. Plugin implementations should extend this class and add custom logic as we will see further in this document with a more advanced example. class PipeElement(ManagedService): \"\"\"The basic building block of an Ambianic pipeline.\"\"\" def __init__(self): super().__init__() self._state = PIPE_STATE_STOPPED self._next_element = None self._latest_heartbeat = time.monotonic() @property def state(self): \"\"\"Lifecycle state of the pipe element.\"\"\" return self._state def start(self): \"\"\"Only sourcing elements (first in a pipeline) need to override. It is invoked once when the enclosing pipeline is started. It should continue to run until the corresponding stop() method is invoked on the same object from a separate pipeline lifecycle manager thread. It is recommended for overriding methods to invoke this base method via super().start() before proceeding with custom logic. \"\"\" self._state = PIPE_STATE_RUNNING @abc.abstractmethod def heal(self): # pragma: no cover \"\"\"Override with adequate implementation of a healing procedure. heal() is invoked by a lifecycle manager when its determined that the element does not respond within reasonable timeframe. This can happen for example if the element depends on external IO resources, which become unavailable for an extended period of time. The healing procedure should be considered a chance to recover or find an alternative way to proceed. If heal does not reset the pipe element back to a responsive state, it is likely that the lifecycle manager will stop the element and its ecnlosing pipeline. \"\"\" pass def healthcheck(self): \"\"\"Check the health of this element. :returns: (timestamp, status) tuple with most recent heartbeat timestamp and health status code ('OK' normally). \"\"\" status = 'OK' # At some point status may carry richer information return self._latest_heartbeat, status def heartbeat(self): \"\"\"Set the heartbeat timestamp to time.monotonic(). Keeping the heartbeat timestamp current informs the lifecycle manager that this element is functioning well. \"\"\" now = time.monotonic() self._latest_heartbeat = now def stop(self): \"\"\"Receive stop signal and act accordingly. Subclasses implementing sourcing elements should override this method by first invoking their super class implementation and then running through steps specific to stopping their ongoing sample processing. \"\"\" self._state = PIPE_STATE_STOPPED def connect_to_next_element(self, next_element=None): \"\"\"Connect this element to the next element in the pipe. Subclasses should not override this method. \"\"\" assert next_element assert isinstance(next_element, PipeElement) self._next_element = next_element def receive_next_sample(self, **sample): \"\"\"Receive next sample from a connected previous element if applicable. All pipeline elements except for the first (sourcing) element in the pipeline will depend on this method to feed them with new samples to process. Subclasses should not override this method. :Parameters: ---------- **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. \"\"\" self.heartbeat() for processed_sample in self.process_sample(**sample): if self._next_element: if (processed_sample): self._next_element.receive_next_sample(**processed_sample) else: self._next_element.receive_next_sample() self.heartbeat() @abc.abstractmethod # pragma: no cover def process_sample(self, **sample) -> Iterable[dict]: \"\"\"Override and implement as generator. Invoked by receive_next_sample() when the previous element (or pipeline source) feeds another data input sample. Implementing subclasses should process input samples and yield output samples for the next element in the pipeline. :Parameters: ---------- **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. :Returns: ---------- processed_sample: Iterable[dict] Generates processed samples to be passed on to the next pipeline element. \"\"\" yield sample The following sequence diagram illustrates an example flow of data from an external security camera source through VideoSourceEdge and the rest of an object detection pipeline. sequenceDiagram participant Cam as Security Camera participant Source as VideoSourceEdge participant Detect as ObjectDetection participant Store as StoreDetection participant File as File System loop Until pipeline.stop() Cam->>Source: rtsp://ip/video/channel0 Source->>Detect: process_sample(image) Detect->>Store: process_sample(det_boxes) Store->>File: file.write(image, det_json) end","title":"A basic edge plugin template"},{"location":"developers/writing-plugins-overview/#a-basic-ui-plugin-template","text":"","title":"A basic UI plugin template"},{"location":"developers/writing-plugins-overview/#a-basic-plugin-bundle-template","text":"","title":"A basic plugin bundle template"},{"location":"developers/writing-plugins-overview/#a-more-advanced-plugin-example","text":"","title":"A more advanced plugin example"},{"location":"developers/writing-plugins-overview/#required-components-of-a-plugin","text":"","title":"Required components of a plugin"},{"location":"developers/writing-plugins-overview/#optional-components-of-a-plugin","text":"","title":"Optional components of a plugin"},{"location":"developers/writing-plugins-overview/#writing-a-custom-plugins-step-by-step","text":"","title":"Writing a custom plugins: step by step"},{"location":"developers/writing-plugins-overview/#testing-plugins","text":"","title":"Testing plugins"},{"location":"developers/writing-plugins-overview/#packaging-plugins","text":"","title":"Packaging plugins"},{"location":"developers/writing-plugins-overview/#packaging-and-edge-plugin","text":"Use a standard python wheel and publish on Pypi","title":"Packaging and Edge plugin"},{"location":"developers/writing-plugins-overview/#packaging-and-ui-plugin","text":"Use a standard npm package format and publish on npm","title":"Packaging and UI plugin"},{"location":"developers/writing-plugins-overview/#packaging-a-bundle-of-plugins","text":"In the UI plugin, provide reference to the edge plugin package: PyPi wheel name package version","title":"Packaging a bundle of plugins"},{"location":"developers/writing-plugins-overview/#documenting-plugins","text":"","title":"Documenting plugins"},{"location":"developers/writing-plugins-overview/#publishing-plugins","text":"More about Publishing your AI app","title":"Publishing plugins"},{"location":"legal/CONTRIBUTING/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the Apache 2.0 license; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the Apache 2.0 license; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again. Attribution The text of this license is available under the Creative Commons Attribution-ShareAlike 3.0 Unported License. It is based on the Home Assistant CLA, which was in turn derived from the Linux Developer Certificate Of Origin, but modified to explicitly use the Apache 2.0 license and not mention sign-off. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Please follow these best practices: To expedite the review process, please keep PRs scoped down Link PRs to specific issues they intend to resolve. Make sure that your PR improves code coverage in a meaningful way. PRs that decrease code coverage will not be allowed. Consult GitHub Help for more information on using pull requests. Semantic Releases Ambianic.ai adheres to the Semantic Release conventions . All package releases (to npm, pypi, docker) are completely automated in the Continuous Integration and Deployment scripts, which trigger on every Pull Request and commit to a project master branch. Community Guidelines This project follows Google's Open Source Community Guidelines . Setup Your Development Environment Now that you are familiar with the contribution guidelines, you are ready to start coding. Follow this guide to set up your dev environment .","title":"Contribution Guidelines"},{"location":"legal/CONTRIBUTING/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"legal/CONTRIBUTING/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the Apache 2.0 license; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the Apache 2.0 license; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"legal/CONTRIBUTING/#attribution","text":"The text of this license is available under the Creative Commons Attribution-ShareAlike 3.0 Unported License. It is based on the Home Assistant CLA, which was in turn derived from the Linux Developer Certificate Of Origin, but modified to explicitly use the Apache 2.0 license and not mention sign-off.","title":"Attribution"},{"location":"legal/CONTRIBUTING/#code-reviews","text":"All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Please follow these best practices: To expedite the review process, please keep PRs scoped down Link PRs to specific issues they intend to resolve. Make sure that your PR improves code coverage in a meaningful way. PRs that decrease code coverage will not be allowed. Consult GitHub Help for more information on using pull requests.","title":"Code reviews"},{"location":"legal/CONTRIBUTING/#semantic-releases","text":"Ambianic.ai adheres to the Semantic Release conventions . All package releases (to npm, pypi, docker) are completely automated in the Continuous Integration and Deployment scripts, which trigger on every Pull Request and commit to a project master branch.","title":"Semantic Releases"},{"location":"legal/CONTRIBUTING/#community-guidelines","text":"This project follows Google's Open Source Community Guidelines .","title":"Community Guidelines"},{"location":"legal/CONTRIBUTING/#setup-your-development-environment","text":"Now that you are familiar with the contribution guidelines, you are ready to start coding. Follow this guide to set up your dev environment .","title":"Setup Your Development Environment"},{"location":"users/ai-apps-bazaar/","text":"About the AI Apps Bazaar Github Repo URL Docker hub URL?","title":"About the AI Apps Bazaar"},{"location":"users/ai-apps-bazaar/#about-the-ai-apps-bazaar","text":"Github Repo URL Docker hub URL?","title":"About the AI Apps Bazaar"},{"location":"users/ai-apps/","text":"Ambianic AI Apps Standard apps Custom apps Apps Bazaar","title":"Ambianic AI Apps"},{"location":"users/ai-apps/#ambianic-ai-apps","text":"Standard apps Custom apps Apps Bazaar","title":"Ambianic AI Apps"},{"location":"users/ambianicbox/","text":"Ambianic Box Ambianic Box is the recommended enclosure for DIY (do-it-yourself) installations of Ambianic Edge. It is a 3D enclosure for Raspberry Pi 4B with a Raspberry Pi Camera. Why does Ambianic Box exist? Proprietary camera manufacturers don't make it easy to access their products via open APIs. RTSP is available in some models, but hard to figure out. ONVIF is partially supported and usually hidden behind several proprietary enablement steps. Turned off by defauly more often than not. It often requires installing custom, unsupported firmware in order to open RTSP or RTMP access. And there is no telling if and when that option may be discontinued. It depends on company policy. Most camera manufacturers want you to use their own app for access to their cameras and integration into their own home automation platforms. They are protecting current and future revenue. Trying to please multiple masters with different demands: end users who want a great product and investors who want great ROI. That creates a trend to move towards centralized cloud subscription and data collection services which have been proven prone to hacks and massive user data leaks. It is hard to build trust with users without verifiable code transparency proving protection of user data privacy. So here we go. Let's build something that works for Open Source developers and DIY folks. You can 3D Print an Ambianic Box at home ( here is the source ) or order a 3D print online through a service such as craftcloud3d.com or makexyz.com . Alternatively, if you don't want to 3D print an enclosure, you can order a kit such as this ony by Labists which includes a usable enclosure box and other components required to build an Ambianic Edge device. Components To assemble a fully functional Ambianic Box, we recommend the following readily available off the shelf components, but you can use any viable alternatives available to you: Component Retail Price Link(s) to Online Stores Raspberry Pi 4 B 2GB $35 RaspberryPi.org , amazon SD Card 32GB $8 amazon , Best Buy 5V 3A USBC Charger $3 wish , ebay , amazon 5MP generic picam $5 aliexpress , amazon 3D Print Material* (130g of PLA) $3 MatterHackers , amazon Total $54 _ _____ *3D Print Note If you don't have a 3D printer handy for the box enclosure, you can use an online service, in which case the cost for the enclosue will vary from $20 to $60 depending on time, distance, material and other options. How to Assemble an Ambianic Box Instructional Video .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Slides .responsive-google-slides { position: relative; padding-bottom: 56.25%; /* 16:9 Ratio */ height: 0; overflow: hidden; } .responsive-google-slides iframe { border: 0; position: absolute; top: 0; left: 0; width: 100% !important; height: 100% !important; } Wall Mount Note Ambianic Box can stand up on a flat surface or it can be mounted. For wall mount, there are several options. The box has two openings on the back side that can be attached to wall mount hooks. Alternatively if you don't feel like drilling into your wall, the box is light enough to mount via two sided mounting tape such as this one . Installing Ambianic Edge Ambianic Box comes to live once Ambianic Edge software is installed. Follow this step by step install guide .","title":"Ambianic Box"},{"location":"users/ambianicbox/#ambianic-box","text":"Ambianic Box is the recommended enclosure for DIY (do-it-yourself) installations of Ambianic Edge. It is a 3D enclosure for Raspberry Pi 4B with a Raspberry Pi Camera.","title":"Ambianic Box"},{"location":"users/ambianicbox/#why-does-ambianic-box-exist","text":"Proprietary camera manufacturers don't make it easy to access their products via open APIs. RTSP is available in some models, but hard to figure out. ONVIF is partially supported and usually hidden behind several proprietary enablement steps. Turned off by defauly more often than not. It often requires installing custom, unsupported firmware in order to open RTSP or RTMP access. And there is no telling if and when that option may be discontinued. It depends on company policy. Most camera manufacturers want you to use their own app for access to their cameras and integration into their own home automation platforms. They are protecting current and future revenue. Trying to please multiple masters with different demands: end users who want a great product and investors who want great ROI. That creates a trend to move towards centralized cloud subscription and data collection services which have been proven prone to hacks and massive user data leaks. It is hard to build trust with users without verifiable code transparency proving protection of user data privacy. So here we go. Let's build something that works for Open Source developers and DIY folks. You can 3D Print an Ambianic Box at home ( here is the source ) or order a 3D print online through a service such as craftcloud3d.com or makexyz.com . Alternatively, if you don't want to 3D print an enclosure, you can order a kit such as this ony by Labists which includes a usable enclosure box and other components required to build an Ambianic Edge device.","title":"Why does Ambianic Box exist?"},{"location":"users/ambianicbox/#components","text":"To assemble a fully functional Ambianic Box, we recommend the following readily available off the shelf components, but you can use any viable alternatives available to you: Component Retail Price Link(s) to Online Stores Raspberry Pi 4 B 2GB $35 RaspberryPi.org , amazon SD Card 32GB $8 amazon , Best Buy 5V 3A USBC Charger $3 wish , ebay , amazon 5MP generic picam $5 aliexpress , amazon 3D Print Material* (130g of PLA) $3 MatterHackers , amazon Total $54 _ _____ *3D Print Note If you don't have a 3D printer handy for the box enclosure, you can use an online service, in which case the cost for the enclosue will vary from $20 to $60 depending on time, distance, material and other options.","title":"Components"},{"location":"users/ambianicbox/#how-to-assemble-an-ambianic-box","text":"","title":"How to Assemble an Ambianic Box"},{"location":"users/ambianicbox/#instructional-video","text":".embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }","title":"Instructional Video"},{"location":"users/ambianicbox/#slides","text":".responsive-google-slides { position: relative; padding-bottom: 56.25%; /* 16:9 Ratio */ height: 0; overflow: hidden; } .responsive-google-slides iframe { border: 0; position: absolute; top: 0; left: 0; width: 100% !important; height: 100% !important; } Wall Mount Note Ambianic Box can stand up on a flat surface or it can be mounted. For wall mount, there are several options. The box has two openings on the back side that can be attached to wall mount hooks. Alternatively if you don't feel like drilling into your wall, the box is light enough to mount via two sided mounting tape such as this one .","title":"Slides"},{"location":"users/ambianicbox/#installing-ambianic-edge","text":"Ambianic Box comes to live once Ambianic Edge software is installed. Follow this step by step install guide .","title":"Installing Ambianic Edge"},{"location":"users/ambianicedge/","text":"Ambianic Edge Ambianic Edge is an Open Source software project that turns your inexpensive home computer (Raspberry Pi or an old laptop) into a smart camera device. We recommend running Ambianic Edge on Ambianic Box hardware but you can also install and run it on most ARM on x86 computers. Quick Setup We are meticulously focused on a simplified and streamlined DIY user experience from hardware assembly to mobile app. However we welcome advanced users to bring in their own components as needed. Hardware The easiest way to start is by assembling a new Ambianic Box from inexpensive off the shelf components. Just follow this step by step guide . Software Follow the step by step instructions for a quick software install available here . Once installed and running, you can connect to an Ambianic Edge via the UI app and see a timeline of detection events. Advanced Setup Hardware You can use your own home computer as long as it has at least as much computing power as the reference hardware used for an Ambianic Box Software Follow these Advanced Install and CLI access instructions available here . Configuration Some configuration features are available in the UI but more advanced configuration requires logging into the Ambianic Edge device via ssh and editing its config.yaml file. Read more about advanced configuration. How to run in development mode If you are interested to try the development version, follow this guide . Project Status and Stats Acknowledgements This project has been inspired by the prior work of many bright people. Special gratitude to: * Yong Tang for his guidance as Tensorflow SIG IO project lead * Robin Cole for his invaluable insights and code on home automation AI with Home Assistant * Blake Blackshear for his work on Frigate and vision for the home automation AI space","title":"Ambianic Edge"},{"location":"users/ambianicedge/#ambianic-edge","text":"Ambianic Edge is an Open Source software project that turns your inexpensive home computer (Raspberry Pi or an old laptop) into a smart camera device. We recommend running Ambianic Edge on Ambianic Box hardware but you can also install and run it on most ARM on x86 computers.","title":"Ambianic Edge"},{"location":"users/ambianicedge/#quick-setup","text":"We are meticulously focused on a simplified and streamlined DIY user experience from hardware assembly to mobile app. However we welcome advanced users to bring in their own components as needed.","title":"Quick Setup"},{"location":"users/ambianicedge/#hardware","text":"The easiest way to start is by assembling a new Ambianic Box from inexpensive off the shelf components. Just follow this step by step guide .","title":"Hardware"},{"location":"users/ambianicedge/#software","text":"Follow the step by step instructions for a quick software install available here . Once installed and running, you can connect to an Ambianic Edge via the UI app and see a timeline of detection events.","title":"Software"},{"location":"users/ambianicedge/#advanced-setup","text":"","title":"Advanced Setup"},{"location":"users/ambianicedge/#hardware_1","text":"You can use your own home computer as long as it has at least as much computing power as the reference hardware used for an Ambianic Box","title":"Hardware"},{"location":"users/ambianicedge/#software_1","text":"Follow these Advanced Install and CLI access instructions available here .","title":"Software"},{"location":"users/ambianicedge/#configuration","text":"Some configuration features are available in the UI but more advanced configuration requires logging into the Ambianic Edge device via ssh and editing its config.yaml file. Read more about advanced configuration.","title":"Configuration"},{"location":"users/ambianicedge/#how-to-run-in-development-mode","text":"If you are interested to try the development version, follow this guide .","title":"How to run in development mode"},{"location":"users/ambianicedge/#project-status-and-stats","text":"","title":"Project Status and Stats"},{"location":"users/ambianicedge/#acknowledgements","text":"This project has been inspired by the prior work of many bright people. Special gratitude to: * Yong Tang for his guidance as Tensorflow SIG IO project lead * Robin Cole for his invaluable insights and code on home automation AI with Home Assistant * Blake Blackshear for his work on Frigate and vision for the home automation AI space","title":"Acknowledgements"},{"location":"users/ambianicos/","text":"Ambianic OS Ambianic OS provides a convenient binary image for Raspberry Pi 4B with Ambianic Edge pre-installed. The released image files are ready to be flashed on to an SD card and plugged into an rpi. When the raspberry pi boots for the first time with the new image on its SD Card, it runs a WiFi captive portal that allows the user to connect to it and setup WiFi access for the rpi. Once the rpi connects to the internet it is ready to be accessed and managed by the Ambianic UI PWA. Tested on Raspberry Pi 4 B Built on Raspberry Pi OS Lite via pi-gen . Simple WiFi Setup via Balena Wifi Connect Visual configuration and management via Ambianic UI Recommended enclosure: Ambianic Box This image automatically and continuously updates to the latest version of Ambianic Edge using docker watchtower . Following are instructions how to install Ambianic OS on your Ambianic Box or another Raspberry Pi device. Flash to SD Card The easiest way to flash the Ambianic Edge Raspberry Pi Image to your SD card is to use Etcher . Download and install the latest version of Etcher . Open Etcher and select Flash from URL then point to the latest Ambianic Edge Raspberry Pi image . There is no need to download the image first. Choose the drive your SD card has been inserted into. Click Flash. First Boot / Network Setup Now that you have flashed your SD card, you can insert it into your Raspberry Pi. WiFi Setup Follow these steps to connect your device to WiFi: Power on your device without an Ethernet cable attached. Wait 1-2 minutes Use your mobile phone to scan for new WiFi networks Connect to the hotspot named Ambianic Edge WiFi Setup . Wait a few moments until the captive portal opens, this portal will allow you to connect the Raspberry Pi to your local WiFi network. If you enter your WiFi credentials incorrectly the Ambianic Edge WiFi Setup hotspot will reappear allowing you to try again. You should see screens similar to the ones below. Ambianic UI connection Once the WiFi connection is established, the Raspberry Pi will pull the latest Ambianic Edge binaries. This may take 5-10 minutes based on your internet connection speed. By default the device is configured to use a Raspberry Pi Camera as input source and report person detection events. After the update process is completed, you can proceed to pair the edge device with your Ambianic UI app. The detailed steps to pair Ambianic UI to Ambianic Edge are described here . Troubleshooting Raspberry Pi is a DYI platform and it requires some technical skills to operate properly. If you don't have any prior experience with Raspberry Pi and picam, you should take some time to learn the basics. Power Supply Raspberry Pi 4 requires a specific 5V 3A power supply, which is different from previous versions of the pi. If you use the wrong type, you may hurt your pi device. Picam If your picamera is not properly connected to the Raspberry Pi you will have trouble using it with Ambianic. Here is a great introductory tutorial on connecting and testing picam . Developing and Debugging If you are looking to develop and debug Ambianic Edge code on Raspberry Pi, you can login to the newly booted device from this image via ssh. On your WiFi network, the device will show up with a host name of ambianic-edge. On most networks you should be able to ping ambianic-edge.lan If you have multiple Ambianic Edge devices running on the same local network, you should use their unique IP address to connect to each. After you login via ssh, you can change the host names to something unique: ambianic-edge1, ambianic-edge2, etc. You can login to a device via ssh with the ambianic-edge user name and an initial and temporary password ChangeMeOnFirstLogin . ssh ambianic-edge@ambianic-edge.lan Notice that you will use ambianic-edge as an user name instead of pi . You will find Ambianic Edge binaries and data files under /opt/ambianic and config files under /etc/ambianic . Project Status and Stats Other notes The awesome Homebridge project provided inspiration and reference code for the method chosen to build the Ambianic Edge Raspberry Pi image. A new Ambianic Edge rpi image is released about once a year to keep up with underlying OS file dependencies. The Ambianic Edge rpi image is not updated each time a new version of the Ambianic Edge software is released. Instead the rpi image uses docker to run the ambianic-edge application software and it automatically updates the docker images using docker watchtower when they are released. This greatly reduces the need for manual maintenance of Ambianic Edge devices. The targeted lifespan of an Ambianic Edge image is 4-5 years. Once an Ambianic Edge device is up and running, it should not require manual intervention for software updates throughout its target lifespan of 4-5 years.","title":"Ambianic OS"},{"location":"users/ambianicos/#ambianic-os","text":"Ambianic OS provides a convenient binary image for Raspberry Pi 4B with Ambianic Edge pre-installed. The released image files are ready to be flashed on to an SD card and plugged into an rpi. When the raspberry pi boots for the first time with the new image on its SD Card, it runs a WiFi captive portal that allows the user to connect to it and setup WiFi access for the rpi. Once the rpi connects to the internet it is ready to be accessed and managed by the Ambianic UI PWA. Tested on Raspberry Pi 4 B Built on Raspberry Pi OS Lite via pi-gen . Simple WiFi Setup via Balena Wifi Connect Visual configuration and management via Ambianic UI Recommended enclosure: Ambianic Box This image automatically and continuously updates to the latest version of Ambianic Edge using docker watchtower . Following are instructions how to install Ambianic OS on your Ambianic Box or another Raspberry Pi device.","title":"Ambianic OS"},{"location":"users/ambianicos/#flash-to-sd-card","text":"The easiest way to flash the Ambianic Edge Raspberry Pi Image to your SD card is to use Etcher . Download and install the latest version of Etcher . Open Etcher and select Flash from URL then point to the latest Ambianic Edge Raspberry Pi image . There is no need to download the image first. Choose the drive your SD card has been inserted into. Click Flash.","title":"Flash to SD Card"},{"location":"users/ambianicos/#first-boot-network-setup","text":"Now that you have flashed your SD card, you can insert it into your Raspberry Pi.","title":"First Boot / Network Setup"},{"location":"users/ambianicos/#wifi-setup","text":"Follow these steps to connect your device to WiFi: Power on your device without an Ethernet cable attached. Wait 1-2 minutes Use your mobile phone to scan for new WiFi networks Connect to the hotspot named Ambianic Edge WiFi Setup . Wait a few moments until the captive portal opens, this portal will allow you to connect the Raspberry Pi to your local WiFi network. If you enter your WiFi credentials incorrectly the Ambianic Edge WiFi Setup hotspot will reappear allowing you to try again. You should see screens similar to the ones below.","title":"WiFi Setup"},{"location":"users/ambianicos/#ambianic-ui-connection","text":"Once the WiFi connection is established, the Raspberry Pi will pull the latest Ambianic Edge binaries. This may take 5-10 minutes based on your internet connection speed. By default the device is configured to use a Raspberry Pi Camera as input source and report person detection events. After the update process is completed, you can proceed to pair the edge device with your Ambianic UI app. The detailed steps to pair Ambianic UI to Ambianic Edge are described here .","title":"Ambianic UI connection"},{"location":"users/ambianicos/#troubleshooting","text":"Raspberry Pi is a DYI platform and it requires some technical skills to operate properly. If you don't have any prior experience with Raspberry Pi and picam, you should take some time to learn the basics.","title":"Troubleshooting"},{"location":"users/ambianicos/#power-supply","text":"Raspberry Pi 4 requires a specific 5V 3A power supply, which is different from previous versions of the pi. If you use the wrong type, you may hurt your pi device.","title":"Power Supply"},{"location":"users/ambianicos/#picam","text":"If your picamera is not properly connected to the Raspberry Pi you will have trouble using it with Ambianic. Here is a great introductory tutorial on connecting and testing picam .","title":"Picam"},{"location":"users/ambianicos/#developing-and-debugging","text":"If you are looking to develop and debug Ambianic Edge code on Raspberry Pi, you can login to the newly booted device from this image via ssh. On your WiFi network, the device will show up with a host name of ambianic-edge. On most networks you should be able to ping ambianic-edge.lan If you have multiple Ambianic Edge devices running on the same local network, you should use their unique IP address to connect to each. After you login via ssh, you can change the host names to something unique: ambianic-edge1, ambianic-edge2, etc. You can login to a device via ssh with the ambianic-edge user name and an initial and temporary password ChangeMeOnFirstLogin . ssh ambianic-edge@ambianic-edge.lan Notice that you will use ambianic-edge as an user name instead of pi . You will find Ambianic Edge binaries and data files under /opt/ambianic and config files under /etc/ambianic .","title":"Developing and Debugging"},{"location":"users/ambianicos/#project-status-and-stats","text":"","title":"Project Status and Stats"},{"location":"users/ambianicos/#other-notes","text":"The awesome Homebridge project provided inspiration and reference code for the method chosen to build the Ambianic Edge Raspberry Pi image. A new Ambianic Edge rpi image is released about once a year to keep up with underlying OS file dependencies. The Ambianic Edge rpi image is not updated each time a new version of the Ambianic Edge software is released. Instead the rpi image uses docker to run the ambianic-edge application software and it automatically updates the docker images using docker watchtower when they are released. This greatly reduces the need for manual maintenance of Ambianic Edge devices. The targeted lifespan of an Ambianic Edge image is 4-5 years. Once an Ambianic Edge device is up and running, it should not require manual intervention for software updates throughout its target lifespan of 4-5 years.","title":"Other notes"},{"location":"users/ambianicui/","text":"Ambianic UI app Ambianic UI is a modern prorgressive web application (PWA) that provides Plug-and-Play pairing and remote access to an Ambianic Edge device. Below is an example screenshot timeline of events detected by a connected Ambianic Edge device. The app is explicitly designed with user privacy and data ownership in mind: Stores data exclusively on user's own client device (desktop or mobile). Does not store ANY user information in the cloud. User may explicitly chose to store a backup of their data on a server of their choice. User data remains 100% owned and controlled by the user at all times. No fine print. No obscure opt-in / opt-out pop-ups of any kind. Try it now! A hosted version of the app is now available to install here: https://ui.ambianic.ai/ Example screenshots on mobile and desktop: Connecting to an Ambianic Edge device Local Discovery You can easily discover and connect to an Ambianic Box that runs on your local WiFi network. It works similar to Airdrop. Remote Connection You can also connect to an Ambinic Edge device running on a remote network as long as you have access to the device' unique Peer ID. Steps Go to Settings and follow the steps for adding a new device. After a few moments, pairing will conclude and you will see the unique identifier of your Ambianic Edge device. Give your device a unique friendly name such as Entry Area or Front Door. You can now head to the Timeline view and you will be able to see an image of what is in front of the Ambianic Box. The box is pre-configured to detect people and cars. The pairing information is persisted on your Ambianic UI client device and you can now access Ambianic Edge from anywhere remotely! The connection is direct and encrypted end-to-end. When you are ready to explore more advanced capabilities, continue to the next section. Configuration Ambianic provides flexible configuration options via a configuration YAML file. You can customize: pipelines, input sources, AI models, notification channels and more. Read on: Configuring Ambianic Edge . Cloud deployment This project is deployed on Netlify by default and is available at https://ui.ambianic.ai . If you prefer to launch your own deployment from this repo, click the button below: Development Environment For instructions how to build, test and debug Ambianic UI via a virtual Continuous Dev Environment or a local dev environment, see this guide . Project Status and Stats Community Support If you have questions, ideas or cool projects you'd like to share with the Ambianic team and community, please use the Ambianic Twitter channel . Acknowledgements This project's initial version was inspired by David Garoro 's PWA example . The project relies heavily on the awesome Vue.js framework with Vuetify material design components.","title":"Ambianic UI"},{"location":"users/ambianicui/#ambianic-ui-app","text":"Ambianic UI is a modern prorgressive web application (PWA) that provides Plug-and-Play pairing and remote access to an Ambianic Edge device. Below is an example screenshot timeline of events detected by a connected Ambianic Edge device. The app is explicitly designed with user privacy and data ownership in mind: Stores data exclusively on user's own client device (desktop or mobile). Does not store ANY user information in the cloud. User may explicitly chose to store a backup of their data on a server of their choice. User data remains 100% owned and controlled by the user at all times. No fine print. No obscure opt-in / opt-out pop-ups of any kind.","title":"Ambianic UI app"},{"location":"users/ambianicui/#try-it-now","text":"A hosted version of the app is now available to install here: https://ui.ambianic.ai/ Example screenshots on mobile and desktop:","title":"Try it now!"},{"location":"users/ambianicui/#connecting-to-an-ambianic-edge-device","text":"","title":"Connecting to an Ambianic Edge device"},{"location":"users/ambianicui/#local-discovery","text":"You can easily discover and connect to an Ambianic Box that runs on your local WiFi network. It works similar to Airdrop.","title":"Local Discovery"},{"location":"users/ambianicui/#remote-connection","text":"You can also connect to an Ambinic Edge device running on a remote network as long as you have access to the device' unique Peer ID.","title":"Remote Connection"},{"location":"users/ambianicui/#steps","text":"Go to Settings and follow the steps for adding a new device. After a few moments, pairing will conclude and you will see the unique identifier of your Ambianic Edge device. Give your device a unique friendly name such as Entry Area or Front Door. You can now head to the Timeline view and you will be able to see an image of what is in front of the Ambianic Box. The box is pre-configured to detect people and cars. The pairing information is persisted on your Ambianic UI client device and you can now access Ambianic Edge from anywhere remotely! The connection is direct and encrypted end-to-end. When you are ready to explore more advanced capabilities, continue to the next section.","title":"Steps"},{"location":"users/ambianicui/#configuration","text":"Ambianic provides flexible configuration options via a configuration YAML file. You can customize: pipelines, input sources, AI models, notification channels and more. Read on: Configuring Ambianic Edge .","title":"Configuration"},{"location":"users/ambianicui/#cloud-deployment","text":"This project is deployed on Netlify by default and is available at https://ui.ambianic.ai . If you prefer to launch your own deployment from this repo, click the button below:","title":"Cloud deployment"},{"location":"users/ambianicui/#development-environment","text":"For instructions how to build, test and debug Ambianic UI via a virtual Continuous Dev Environment or a local dev environment, see this guide .","title":"Development Environment"},{"location":"users/ambianicui/#project-status-and-stats","text":"","title":"Project Status and Stats"},{"location":"users/ambianicui/#community-support","text":"If you have questions, ideas or cool projects you'd like to share with the Ambianic team and community, please use the Ambianic Twitter channel .","title":"Community Support"},{"location":"users/ambianicui/#acknowledgements","text":"This project's initial version was inspired by David Garoro 's PWA example . The project relies heavily on the awesome Vue.js framework with Vuetify material design components.","title":"Acknowledgements"},{"location":"users/configure/","text":"Ambianic Configuration Ambianic Edge executes pipelines with input sources, AI inference models and output targets. Ambianic UI visualizes observations in a chronological timeline. Pipelines are configured in a config.yaml file which resides in the runtime directory of the Ambianic Edge docker image. The following diagram illustrates an example pipeline that takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart(); Pipeline Configuration Create a config.yaml file in the workspace directory of the Ambianic Edge docker image. For example, the directory could be named /opt/ambianid-edge.workspace . You can use this starting config.yaml template and modify it to fit your environment. Read further about the parameters and format of the config file. Let's take an example config.yaml file and walk through it: ###################################### # Ambianic main configuration file # ###################################### version: '2020.12.11' # path to the data directory # ./data is relative to the container runtime. Do not change it if you are unsure. data_dir: ./data # Set logging level to one of DEBUG, INFO, WARNING, ERROR logging: file: ./data/ambianic-log.txt level: INFO Notifications provider configuration # see https://github.com/caronc/apprise#popular-notification-services for syntax examples notifications: catch_all_email: include_attachments: true providers: - mailto://userid:pass@domain.com - hassios://{host}/{access_token} alert_fall: providers: - mailto://userid:pass@domain.com - json://hostname/a/path/to/post/to # Pipeline event timeline configuration timeline: event_log: ./data/timeline-event-log.yaml # Cameras and other input data sources sources: # direct support for raspberry picamera picamera: uri: picamera type: video live: true front_door_camera: uri: <your camera> type: video live: true # local video device integration example webcam: uri: /dev/video0 type: video live: true # recorded front door cam feed for quick testing recorded_cam_feed: uri: file:///workspace/tests/pipeline/avsource/test2-cam-person1.mkv type: video ai_models: image_detection: model: tflite: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/coco_labels.txt face_detection: model: tflite: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_face_quant_postprocess.tflite edgetpu: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/coco_labels.txt top_k: 2 fall_detection: model: tflite: /opt/ambianic-edge/ai_models/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite edgetpu: /opt/ambianic-edge/ai_models/posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/pose_labels.txt # A named pipeline defines an ordered sequence of operations # such as reading from a data source, AI model inference, saving samples and others. pipelines: # Pipeline names could be descriptive, e.g. front_door_watch or entry_room_watch. area_watch: - source: picamera - detect_objects: # run ai inference on the input data ai_model: image_detection confidence_threshold: 0.6 # Watch for any of the labels listed below. The labels must be from the model trained label set. # If no labels are listed, then watch for all model trained labels. label_filter: - person - car - save_detections: # save samples from the inference results positive_interval: 300 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_falls: # look for falls ai_model: fall_detection confidence_threshold: 0.6 - save_detections: # save samples from the inference results positive_interval: 10 idle_interval: 600000 notify: # notify a thirdy party service providers: - alert_fall The only parameter you have to change in order to see a populated timeline in the UI is the source uri. In the section below: front_door_camera: uri: <your camera> Replace <your camera> with your camera still image snapshot URI (recommended). For example: front_door_camera: uri: http://192.168.86.29/cgi-bin/api.cgi?cmd=Snap&channel=0&rs=wuuPhkmUCeI9WG7C&user=admin&password=****** type: image live: true Or you can alternatively plug-in the camera video streaming RTSP URI. For example: front_door_camera: uri: rtsp://admin:password@192.168.1.99/media/video1 type: video live: true Only use RTSP if you run high end hardware and use AI inference in your pipelines that needs video and/or audio data instead of still images. Make sure that the HTTP URI points to a valid image. You can test by simply putting the URI in your web browser. You should see a current snapshot of live camera capture. If you choose to use an RTSP URI, then you can test that with a tool like VLC . The parameter live: true indicates that this is a continuous stream without a predetermined end. Therefore Ambianic Edge should do whatever it can to continuously pull media and recover from interruptions caused by network or other glitches. Now save the file and restart the docker image. Within a few moments you should be able to see a populated timeline in the UI with fresh images from your camera and some object, people and face detections if there are any to detect. You can reference the Quick Start Guide for instructions on starting and stopping the Ambianic Edge docker image. Camera Configuration Cameras are some of the most common sources of input data for Ambianic.ai pipelines. Ambianic.ai is typically connected to IP Network cameras, but you can also connect a local embedded web cam or USB connected camera. You can set uri to a local path such as /dev/video0 to use a working usb webcam or picamera in case you have a picamera connected to Raspbery Pi connector. Connecting a local Web USB camera A laptop camera or USB camera on linux or mac can be used just providing a working reference to the video device, eg. /dev/video0 Connecting a Picamera To use a Picamera connected to the camera connector on the Raspberry Pi (usually available in the middle of the board) Ensure also your setup have the camera enabled and provides enough GPU memory for processing frames. The following script will take care of it for you: # Enable camera sudo raspi-config nonint do_camera 1 # Allocate GPU to camera: if ! grep -Fq \"gpu_mem=\" /boot/config.txt; then echo \"gpu_mem=256\" | sudo tee -a /boot/config.txt fi In a tipical setup your /boot/config.txt should contain something like this at the end [all] start_x=1 gpu_mem=256 Then update your configuration to point to the right source picamera: uri: picamera type: video live: true Connecting over HTTP or RTSP Another option is to stream your local camera over HTTP or RTSP. VLC is one of the most popular and easy to use apps for that. Here is how you can turn your local webcam into an IP streaming camera. The streaming URL shared by VLC is the input source URI for the Ambianic.ai configuration file. How to find the URI for your camera If you don't have experience configuring surveillance cameras, it can be tricky to find the URI. We are working on a camera Plug-and-Play feature in Ambianic to make it easier to discover and connect to your cameras. Keep an eye for release news. In the meanwhile, you have several options: First, check your camera manufacturer's documentation whether it supports still image URL over HTTP or video streaming over RTSP. Most IP cameras do, but not all. If your camera is ONVIF standard conformant, then you will have access to HTTP camera images and RTSP video stream. You can use the ONVIF Search Tool to check if your camera is listed as conformant. You can then use a tool such as ONVIF Device Manager to auto discover your IP camera and show you its RTSP and HTTP URI. Here is a quick How-To . A more detailed post on this tool is also available here . Your camera manufacturer will likely have an online resource describing how to determine its RTSP address. It would look something like this one . There is also an online directory where you can search for the RTSP URI of many camera brands. Using still image source instead of a video stream for your camera URI In many cases processing 1 frame per second is sufficient frequency to detect interesting events in your environment. It is usually more CPU and network resource efficient to use a still image source instead of live RTSP stream for 1fps. This approach allows Ambianic.ai Edge to pull from the camera another image snapshot when it is ready as opposed to streaming which pushes constantly media updates that Edge may or may not be ready to process. All you have to do to use a still image source from your camera is to locate the specific image URI as described above and plug it in the corresponding source section of the config.yaml file. Here is what it would look like: front_door_camera: uri: http://192.168.86.29/cgi-bin/api.cgi?cmd=Snap&channel=0&rs=wuuPhkmUCeI9WG7C&user=admin&password=****** type: image live: true In the example above the URI points to a still jpg sample from the camera. Ambianic Edge will continuously poll this URI approximately once per second (1fps). Sensitive config information Since camera URIs often contain credentials, we recommend that you store such values in secrets.yaml . Ambianic Edge will automatically look for and if available prepend secrets.yaml to config.yaml . That way you can share config.yaml with others without compromising privacy sensitive parameters. You can use standard YAML anchors and aliases in config.yaml to reference YAML variables defined in secrets.yaml . They will resolve after the files are merged into one. An example valid entry in secrets.yaml for a camera URI, would look like this: secret_uri_front_door_camera: &secret_uri_front_door_camera 'rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101' # add more secret entries as regular yaml mappings It can be then referenced in config.yaml as follows: front_door_camera: uri: *secret_uri_front_door_camera Notification settings Ambianic edge can be configured to instantly alerts users when a detection occurs. Notifications are a feature of the save_detections element. Every time a timeline event is saved along with its contextual data, a notification can be fired to a number of supported channels such as email, sms, or a local smart home hub such as Home Assistant ( hassios://{host}/{access_token} ). Notification providers are first configured at a system level using the following syntax: notifications: catch_all_email: include_attachments: true providers: - mailto://userid:pass@domain.com - hassios://{host}/{access_token} alert_fall: providers: - mailto://userid:pass@domain.com - json://hostname/a/path/to/post/to Notice that the catch_all_email provider template uses the include_attachments attribute. Email notifications to this provider will include timeline event attachments such as captured images. On the other hand the alert_fall does not use the include_attachments attribute. Notifications sent through this provider will include the essential event information, but no attachments. In order to include attachments via alert_fall , just add the include_attachments: true element. For a full list of supported providers and config syntax, take a look at the official apprise documentation . Apprise is a great notifications library that ambianic edge uses and sponsors. Once the system level notification providers are configured, they can be referenced within save_detection elements as in the following example: pipelines: # Pipeline names could be descriptive, e.g. front_door_watch or entry_room_watch. area_watch: ... - detect_falls: # look for falls ... - save_detections: # save samples from the inference results positive_interval: 10 idle_interval: 600000 notify: # notify a thirdy party service providers: - alert_fall Each time a detection event is saved, a corresponding notification will be instantly fired. Other configuration settings The rest of the configuration settings are for advanced developers. If you feel ready to dive into log files and code, you can experiement with different AI models and pipeline elements. Otherwise leave them as they are. Using AI Accelerator Ambianic Edge will use the Google Coral TPU accelerator if one is available on the system. Note: The default docker compose configuration for Ambianic Edge checks whether a USB attached Coral is running on /dev/bus/usb . You may need to adjust that if the TPU is attached to a different file system directory. Coral is a powerful TPU that can speed up inference 5-10 times. However AI inference is only part of all the functions that execute in an Ambianic Edge pipeline. Video decoding and formatting also takes substantial amount of processing time. Overall we've seen Coral to improve performance 30-50% on a realistic Raspberry Pi 4 deployment with multiple simultaneous pipelines pulling images from multiple cameras and processing these images with multiple inference models. If you don't have a Coral device available, no need to worry for now. Raspberry Pi 4 is quite powerful. It comfortably handles 3 simultaneous HD camera sourced pipelines with approximately 1-2 frames per second (fps). An objects or person of interest would normally show up in one of your cameras for at least one second, which is enough time to be registered and processed by Ambianic Edge on a plain RPI4. Configuration Questions If you are having difficulties with configuration settings, feel free to ask your question in the community discussion forum or our public slack channel .","title":"Configuration"},{"location":"users/configure/#ambianic-configuration","text":"Ambianic Edge executes pipelines with input sources, AI inference models and output targets. Ambianic UI visualizes observations in a chronological timeline. Pipelines are configured in a config.yaml file which resides in the runtime directory of the Ambianic Edge docker image. The following diagram illustrates an example pipeline that takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart();","title":"Ambianic Configuration"},{"location":"users/configure/#pipeline-configuration","text":"Create a config.yaml file in the workspace directory of the Ambianic Edge docker image. For example, the directory could be named /opt/ambianid-edge.workspace . You can use this starting config.yaml template and modify it to fit your environment. Read further about the parameters and format of the config file. Let's take an example config.yaml file and walk through it: ###################################### # Ambianic main configuration file # ###################################### version: '2020.12.11' # path to the data directory # ./data is relative to the container runtime. Do not change it if you are unsure. data_dir: ./data # Set logging level to one of DEBUG, INFO, WARNING, ERROR logging: file: ./data/ambianic-log.txt level: INFO Notifications provider configuration # see https://github.com/caronc/apprise#popular-notification-services for syntax examples notifications: catch_all_email: include_attachments: true providers: - mailto://userid:pass@domain.com - hassios://{host}/{access_token} alert_fall: providers: - mailto://userid:pass@domain.com - json://hostname/a/path/to/post/to # Pipeline event timeline configuration timeline: event_log: ./data/timeline-event-log.yaml # Cameras and other input data sources sources: # direct support for raspberry picamera picamera: uri: picamera type: video live: true front_door_camera: uri: <your camera> type: video live: true # local video device integration example webcam: uri: /dev/video0 type: video live: true # recorded front door cam feed for quick testing recorded_cam_feed: uri: file:///workspace/tests/pipeline/avsource/test2-cam-person1.mkv type: video ai_models: image_detection: model: tflite: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/coco_labels.txt face_detection: model: tflite: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_face_quant_postprocess.tflite edgetpu: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/coco_labels.txt top_k: 2 fall_detection: model: tflite: /opt/ambianic-edge/ai_models/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite edgetpu: /opt/ambianic-edge/ai_models/posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/pose_labels.txt # A named pipeline defines an ordered sequence of operations # such as reading from a data source, AI model inference, saving samples and others. pipelines: # Pipeline names could be descriptive, e.g. front_door_watch or entry_room_watch. area_watch: - source: picamera - detect_objects: # run ai inference on the input data ai_model: image_detection confidence_threshold: 0.6 # Watch for any of the labels listed below. The labels must be from the model trained label set. # If no labels are listed, then watch for all model trained labels. label_filter: - person - car - save_detections: # save samples from the inference results positive_interval: 300 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_falls: # look for falls ai_model: fall_detection confidence_threshold: 0.6 - save_detections: # save samples from the inference results positive_interval: 10 idle_interval: 600000 notify: # notify a thirdy party service providers: - alert_fall The only parameter you have to change in order to see a populated timeline in the UI is the source uri. In the section below: front_door_camera: uri: <your camera> Replace <your camera> with your camera still image snapshot URI (recommended). For example: front_door_camera: uri: http://192.168.86.29/cgi-bin/api.cgi?cmd=Snap&channel=0&rs=wuuPhkmUCeI9WG7C&user=admin&password=****** type: image live: true Or you can alternatively plug-in the camera video streaming RTSP URI. For example: front_door_camera: uri: rtsp://admin:password@192.168.1.99/media/video1 type: video live: true Only use RTSP if you run high end hardware and use AI inference in your pipelines that needs video and/or audio data instead of still images. Make sure that the HTTP URI points to a valid image. You can test by simply putting the URI in your web browser. You should see a current snapshot of live camera capture. If you choose to use an RTSP URI, then you can test that with a tool like VLC . The parameter live: true indicates that this is a continuous stream without a predetermined end. Therefore Ambianic Edge should do whatever it can to continuously pull media and recover from interruptions caused by network or other glitches. Now save the file and restart the docker image. Within a few moments you should be able to see a populated timeline in the UI with fresh images from your camera and some object, people and face detections if there are any to detect. You can reference the Quick Start Guide for instructions on starting and stopping the Ambianic Edge docker image.","title":"Pipeline Configuration"},{"location":"users/configure/#camera-configuration","text":"Cameras are some of the most common sources of input data for Ambianic.ai pipelines. Ambianic.ai is typically connected to IP Network cameras, but you can also connect a local embedded web cam or USB connected camera. You can set uri to a local path such as /dev/video0 to use a working usb webcam or picamera in case you have a picamera connected to Raspbery Pi connector.","title":"Camera Configuration"},{"location":"users/configure/#connecting-a-local-web-usb-camera","text":"A laptop camera or USB camera on linux or mac can be used just providing a working reference to the video device, eg. /dev/video0","title":"Connecting a local Web USB camera"},{"location":"users/configure/#connecting-a-picamera","text":"To use a Picamera connected to the camera connector on the Raspberry Pi (usually available in the middle of the board) Ensure also your setup have the camera enabled and provides enough GPU memory for processing frames. The following script will take care of it for you: # Enable camera sudo raspi-config nonint do_camera 1 # Allocate GPU to camera: if ! grep -Fq \"gpu_mem=\" /boot/config.txt; then echo \"gpu_mem=256\" | sudo tee -a /boot/config.txt fi In a tipical setup your /boot/config.txt should contain something like this at the end [all] start_x=1 gpu_mem=256 Then update your configuration to point to the right source picamera: uri: picamera type: video live: true","title":"Connecting a Picamera"},{"location":"users/configure/#connecting-over-http-or-rtsp","text":"Another option is to stream your local camera over HTTP or RTSP. VLC is one of the most popular and easy to use apps for that. Here is how you can turn your local webcam into an IP streaming camera. The streaming URL shared by VLC is the input source URI for the Ambianic.ai configuration file.","title":"Connecting over HTTP or RTSP"},{"location":"users/configure/#how-to-find-the-uri-for-your-camera","text":"If you don't have experience configuring surveillance cameras, it can be tricky to find the URI. We are working on a camera Plug-and-Play feature in Ambianic to make it easier to discover and connect to your cameras. Keep an eye for release news. In the meanwhile, you have several options: First, check your camera manufacturer's documentation whether it supports still image URL over HTTP or video streaming over RTSP. Most IP cameras do, but not all. If your camera is ONVIF standard conformant, then you will have access to HTTP camera images and RTSP video stream. You can use the ONVIF Search Tool to check if your camera is listed as conformant. You can then use a tool such as ONVIF Device Manager to auto discover your IP camera and show you its RTSP and HTTP URI. Here is a quick How-To . A more detailed post on this tool is also available here . Your camera manufacturer will likely have an online resource describing how to determine its RTSP address. It would look something like this one . There is also an online directory where you can search for the RTSP URI of many camera brands.","title":"How to find the URI for your camera"},{"location":"users/configure/#using-still-image-source-instead-of-a-video-stream-for-your-camera-uri","text":"In many cases processing 1 frame per second is sufficient frequency to detect interesting events in your environment. It is usually more CPU and network resource efficient to use a still image source instead of live RTSP stream for 1fps. This approach allows Ambianic.ai Edge to pull from the camera another image snapshot when it is ready as opposed to streaming which pushes constantly media updates that Edge may or may not be ready to process. All you have to do to use a still image source from your camera is to locate the specific image URI as described above and plug it in the corresponding source section of the config.yaml file. Here is what it would look like: front_door_camera: uri: http://192.168.86.29/cgi-bin/api.cgi?cmd=Snap&channel=0&rs=wuuPhkmUCeI9WG7C&user=admin&password=****** type: image live: true In the example above the URI points to a still jpg sample from the camera. Ambianic Edge will continuously poll this URI approximately once per second (1fps).","title":"Using still image source instead of a video stream for your camera URI"},{"location":"users/configure/#sensitive-config-information","text":"Since camera URIs often contain credentials, we recommend that you store such values in secrets.yaml . Ambianic Edge will automatically look for and if available prepend secrets.yaml to config.yaml . That way you can share config.yaml with others without compromising privacy sensitive parameters. You can use standard YAML anchors and aliases in config.yaml to reference YAML variables defined in secrets.yaml . They will resolve after the files are merged into one. An example valid entry in secrets.yaml for a camera URI, would look like this: secret_uri_front_door_camera: &secret_uri_front_door_camera 'rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101' # add more secret entries as regular yaml mappings It can be then referenced in config.yaml as follows: front_door_camera: uri: *secret_uri_front_door_camera","title":"Sensitive config information"},{"location":"users/configure/#notification-settings","text":"Ambianic edge can be configured to instantly alerts users when a detection occurs. Notifications are a feature of the save_detections element. Every time a timeline event is saved along with its contextual data, a notification can be fired to a number of supported channels such as email, sms, or a local smart home hub such as Home Assistant ( hassios://{host}/{access_token} ). Notification providers are first configured at a system level using the following syntax: notifications: catch_all_email: include_attachments: true providers: - mailto://userid:pass@domain.com - hassios://{host}/{access_token} alert_fall: providers: - mailto://userid:pass@domain.com - json://hostname/a/path/to/post/to Notice that the catch_all_email provider template uses the include_attachments attribute. Email notifications to this provider will include timeline event attachments such as captured images. On the other hand the alert_fall does not use the include_attachments attribute. Notifications sent through this provider will include the essential event information, but no attachments. In order to include attachments via alert_fall , just add the include_attachments: true element. For a full list of supported providers and config syntax, take a look at the official apprise documentation . Apprise is a great notifications library that ambianic edge uses and sponsors. Once the system level notification providers are configured, they can be referenced within save_detection elements as in the following example: pipelines: # Pipeline names could be descriptive, e.g. front_door_watch or entry_room_watch. area_watch: ... - detect_falls: # look for falls ... - save_detections: # save samples from the inference results positive_interval: 10 idle_interval: 600000 notify: # notify a thirdy party service providers: - alert_fall Each time a detection event is saved, a corresponding notification will be instantly fired.","title":"Notification settings"},{"location":"users/configure/#other-configuration-settings","text":"The rest of the configuration settings are for advanced developers. If you feel ready to dive into log files and code, you can experiement with different AI models and pipeline elements. Otherwise leave them as they are.","title":"Other configuration settings"},{"location":"users/configure/#using-ai-accelerator","text":"Ambianic Edge will use the Google Coral TPU accelerator if one is available on the system. Note: The default docker compose configuration for Ambianic Edge checks whether a USB attached Coral is running on /dev/bus/usb . You may need to adjust that if the TPU is attached to a different file system directory. Coral is a powerful TPU that can speed up inference 5-10 times. However AI inference is only part of all the functions that execute in an Ambianic Edge pipeline. Video decoding and formatting also takes substantial amount of processing time. Overall we've seen Coral to improve performance 30-50% on a realistic Raspberry Pi 4 deployment with multiple simultaneous pipelines pulling images from multiple cameras and processing these images with multiple inference models. If you don't have a Coral device available, no need to worry for now. Raspberry Pi 4 is quite powerful. It comfortably handles 3 simultaneous HD camera sourced pipelines with approximately 1-2 frames per second (fps). An objects or person of interest would normally show up in one of your cameras for at least one second, which is enough time to be registered and processed by Ambianic Edge on a plain RPI4.","title":"Using AI Accelerator"},{"location":"users/configure/#configuration-questions","text":"If you are having difficulties with configuration settings, feel free to ask your question in the community discussion forum or our public slack channel .","title":"Configuration Questions"},{"location":"users/custom-ai-apps/","text":"Installing and Running Custom AI Apps Examples of custom apps Where to get custom apps How to install and run","title":"Installing and Running Custom AI Apps"},{"location":"users/custom-ai-apps/#installing-and-running-custom-ai-apps","text":"Examples of custom apps Where to get custom apps How to install and run","title":"Installing and Running Custom AI Apps"},{"location":"users/edge-pairing/","text":"Establishing initial connection (pairing) between Ambianic UI and Ambianic Edge devices Upon starting the Ambianic UI (Progressive Web App) for the first time, it will automatically start looking in the background for an Ambianic Edge device on your own local network. If there is an active Edge device on your local network, it will be discovered and the UI will pair with it within a few moments. If your UI app is not connected to the Internet, you will see an indication in the top Nav Bar indicating that. Once the UI is connected to an Edge device, it will change to: If you want to connect to an Ambianic Edge device that's not on your local network, you can scroll down to Pair with Remote Ambianic Edge device and enter the PeerJS ID you get from someone who can connect to that device on their local network. Enter the Peer JS ID in the input field and press PAIR REMOTELY . Once connected, click on the link Timeline to see the Edge Device settings. If you want to consequently connect to an Edge Device on your local network, go back to Settings and use DISCOVER ON LOCAL NETWORK This will make sure you get disconnected from the remote Edge device and connect back to your own local network Edge device. Once the UI app establishes an innitial connection with an Edge Device, the UI app (PWA) saves the Edge unique PeerID in the browser local (mobile or desktop) device storage. From that point forward, the UI can re-connect from any Internet location.","title":"Local and Remote pairing"},{"location":"users/edge-pairing/#establishing-initial-connection-pairing-between-ambianic-ui-and-ambianic-edge-devices","text":"Upon starting the Ambianic UI (Progressive Web App) for the first time, it will automatically start looking in the background for an Ambianic Edge device on your own local network. If there is an active Edge device on your local network, it will be discovered and the UI will pair with it within a few moments. If your UI app is not connected to the Internet, you will see an indication in the top Nav Bar indicating that. Once the UI is connected to an Edge device, it will change to: If you want to connect to an Ambianic Edge device that's not on your local network, you can scroll down to Pair with Remote Ambianic Edge device and enter the PeerJS ID you get from someone who can connect to that device on their local network. Enter the Peer JS ID in the input field and press PAIR REMOTELY . Once connected, click on the link Timeline to see the Edge Device settings. If you want to consequently connect to an Edge Device on your local network, go back to Settings and use DISCOVER ON LOCAL NETWORK This will make sure you get disconnected from the remote Edge device and connect back to your own local network Edge device. Once the UI app establishes an innitial connection with an Edge Device, the UI app (PWA) saves the Edge unique PeerID in the browser local (mobile or desktop) device storage. From that point forward, the UI can re-connect from any Internet location.","title":"Establishing initial connection (pairing) between Ambianic UI and Ambianic Edge devices"},{"location":"users/faq/","text":"Frequently Asked Questions Does Ambianic use hardware accelerators for AI inference? Yes. Ambianic currently supports the Google Coral EdgeTPU. The server relies on the Tensorflow Lite Runtime to dynamically detect Coral. If no EdgeTPU is available, AI inference falls back on the CPU. Check your logs for messages indicating EdgeTPU availability. Does Ambianic use hardware accelerators for video processing? Yes. Ambianic relies on gstreamer to dynamically detect and use any available GPU on the host platform for video and image processing. Asked Asked Asked 6. 2. TBD","title":"Frequently Asked Questions"},{"location":"users/faq/#frequently-asked-questions","text":"Does Ambianic use hardware accelerators for AI inference? Yes. Ambianic currently supports the Google Coral EdgeTPU. The server relies on the Tensorflow Lite Runtime to dynamically detect Coral. If no EdgeTPU is available, AI inference falls back on the CPU. Check your logs for messages indicating EdgeTPU availability. Does Ambianic use hardware accelerators for video processing? Yes. Ambianic relies on gstreamer to dynamically detect and use any available GPU on the host platform for video and image processing. Asked Asked Asked 6. 2. TBD","title":"Frequently Asked Questions"},{"location":"users/hass-integration/","text":"Home Assistant Integration Home Assistant is one of the most popular Open Source Home Automation hubs and it is a natural integration target for Ambianic. The integration enables users to implement new home automation flows based on Ambianic detection events. For example: * Turn on lights if a family member walks in the house and its night time. * Trigger the alarm if an unknown person is on private property for more than 1 minute. * Turn off the stove if no person has been detected in the house for over an hour; or * Instantly notify family and caregivers if an elderly loved one falls. Notifications The simplest way to integrate Ambianic with Home Assistant is via notifications. See notifications config guide for details. Add-on Ambianic can also be installed as a Home Assistant Add-on. See details here .","title":"Home Assistant Integration"},{"location":"users/hass-integration/#home-assistant-integration","text":"Home Assistant is one of the most popular Open Source Home Automation hubs and it is a natural integration target for Ambianic. The integration enables users to implement new home automation flows based on Ambianic detection events. For example: * Turn on lights if a family member walks in the house and its night time. * Trigger the alarm if an unknown person is on private property for more than 1 minute. * Turn off the stove if no person has been detected in the house for over an hour; or * Instantly notify family and caregivers if an elderly loved one falls.","title":"Home Assistant Integration"},{"location":"users/hass-integration/#notifications","text":"The simplest way to integrate Ambianic with Home Assistant is via notifications. See notifications config guide for details.","title":"Notifications"},{"location":"users/hass-integration/#add-on","text":"Ambianic can also be installed as a Home Assistant Add-on. See details here .","title":"Add-on"},{"location":"users/howto/","text":"How To TBD","title":"How To"},{"location":"users/howto/#how-to","text":"TBD","title":"How To"},{"location":"users/install/","text":"Installing and Running Ambianic Quickstart reference Various install scenarios: CPU, OS matrix","title":"Installing and Running Ambianic"},{"location":"users/install/#installing-and-running-ambianic","text":"Quickstart reference Various install scenarios: CPU, OS matrix","title":"Installing and Running Ambianic"},{"location":"users/installsoftware/","text":"Install and Run Ambianic Software Ambianic has two main software components: Ambianic Edge and Ambianic UI app. - Ambianic Edge software runs in an Ambianic Box enclosure and is responsible for local monitoring and AI inference. Users do not directly interact with Ambianic Edge. - Ambianic UI app is the front end Progressive Web App that users interact with to manage Ambianic Edge devices and observe timeline events reported by Ambianic Edge devices. The following section provides detailed step by step instructions how to install and run an Ambianic system. Instructional Video .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } Slides .responsive-google-slides { position: relative; padding-bottom: 56.25%; /* 16:9 Ratio */ height: 0; overflow: hidden; } .responsive-google-slides iframe { border: 0; position: absolute; top: 0; left: 0; width: 100% !important; height: 100% !important; }","title":"Install and Run Ambianic Software"},{"location":"users/installsoftware/#install-and-run-ambianic-software","text":"Ambianic has two main software components: Ambianic Edge and Ambianic UI app. - Ambianic Edge software runs in an Ambianic Box enclosure and is responsible for local monitoring and AI inference. Users do not directly interact with Ambianic Edge. - Ambianic UI app is the front end Progressive Web App that users interact with to manage Ambianic Edge devices and observe timeline events reported by Ambianic Edge devices. The following section provides detailed step by step instructions how to install and run an Ambianic system.","title":"Install and Run Ambianic Software"},{"location":"users/installsoftware/#instructional-video","text":".embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }","title":"Instructional Video"},{"location":"users/installsoftware/#slides","text":".responsive-google-slides { position: relative; padding-bottom: 56.25%; /* 16:9 Ratio */ height: 0; overflow: hidden; } .responsive-google-slides iframe { border: 0; position: absolute; top: 0; left: 0; width: 100% !important; height: 100% !important; }","title":"Slides"},{"location":"users/mobileui/","text":"Amianic Mobile UI Registration and login Your Inference Timeline Alerts and Notifications","title":"Amianic Mobile UI"},{"location":"users/mobileui/#amianic-mobile-ui","text":"Registration and login Your Inference Timeline Alerts and Notifications","title":"Amianic Mobile UI"},{"location":"users/quickstart/","text":"Quick Start Guide Recommended Install The simplest way to get started is to follow these steps: Assemble Ambianic Box hardware Install and run Ambianic software If you prefer an alternative hosting platform for your Ambianic Edge device, keep reading about more advanced installation options. Advanced Install If you are familiar with Docker you will be able to install Ambianic Edge in less than 5 minutes on almost any computer with x86 or ARM architecture. Follow the steps below: Ambanic Edge Docker image Ambianic Edge is available as a docker image for ARM and x86 architectures so it can be deployed on most modern machines and operating systems. The reference test system is: Raspberry Pi 4 with 4GB RAM and 32GB SD card . Although docker images are available for most common ARM and x86 machines. To deploy on a Raspberry Pi 4, you will need a recent Raspbian install with Docker and Docker Compose on it. You can install and run the image in the default pi user space on Raspbian. Installer for Raspberry OS and Debian-like linux We have a commodity script that will take care of installing and setting up your system. Run this line to setup wget -qO - https://raw.githubusercontent.com/ambianic/ambianic-quickstart/master/installer.sh | sh After the setup you can find the installation under /opt/ambianic where you can find the configuration files and the data directory (under /opt/ambianic/data ). Ambianic Pipeline configuration will be under /etc/ambianic . The Ambianic Edge CLI The installer will start the service for you. To manage the runtime you can use the ambianic command line. A few examples: Start, Stop or Restart with ambianic [ start | stop | restart ] View the instance status with ambianic status View logs with ambianic logs Open the UI (if you system has a GUI) with ambianic ui Upgrade the installation with ambianic upgrade Ambianic UI app Ambianic UI is a modern prorgressive web application (PWA) that provides Plug-and-Play pairing and remote access to Ambianic Edge devices. Read more about Ambianic UI. Development If you want to contribute to the code read how to setup a development environment Troubleshooting If you experience problems with your initial setup and you can't find a good solution online, feel free to engage the team on Twitter , Slack , or Github discussion forums . Supporting the project If you find value in this project, consider supporting its future success. See the sustainability section.","title":"Quick Start"},{"location":"users/quickstart/#quick-start-guide","text":"","title":"Quick Start Guide"},{"location":"users/quickstart/#recommended-install","text":"The simplest way to get started is to follow these steps: Assemble Ambianic Box hardware Install and run Ambianic software If you prefer an alternative hosting platform for your Ambianic Edge device, keep reading about more advanced installation options.","title":"Recommended Install"},{"location":"users/quickstart/#advanced-install","text":"If you are familiar with Docker you will be able to install Ambianic Edge in less than 5 minutes on almost any computer with x86 or ARM architecture. Follow the steps below:","title":"Advanced Install"},{"location":"users/quickstart/#ambanic-edge-docker-image","text":"Ambianic Edge is available as a docker image for ARM and x86 architectures so it can be deployed on most modern machines and operating systems. The reference test system is: Raspberry Pi 4 with 4GB RAM and 32GB SD card . Although docker images are available for most common ARM and x86 machines. To deploy on a Raspberry Pi 4, you will need a recent Raspbian install with Docker and Docker Compose on it. You can install and run the image in the default pi user space on Raspbian.","title":"Ambanic Edge Docker image"},{"location":"users/quickstart/#installer-for-raspberry-os-and-debian-like-linux","text":"We have a commodity script that will take care of installing and setting up your system. Run this line to setup wget -qO - https://raw.githubusercontent.com/ambianic/ambianic-quickstart/master/installer.sh | sh After the setup you can find the installation under /opt/ambianic where you can find the configuration files and the data directory (under /opt/ambianic/data ). Ambianic Pipeline configuration will be under /etc/ambianic .","title":"Installer for Raspberry OS and Debian-like linux"},{"location":"users/quickstart/#the-ambianic-edge-cli","text":"The installer will start the service for you. To manage the runtime you can use the ambianic command line. A few examples: Start, Stop or Restart with ambianic [ start | stop | restart ] View the instance status with ambianic status View logs with ambianic logs Open the UI (if you system has a GUI) with ambianic ui Upgrade the installation with ambianic upgrade","title":"The Ambianic Edge CLI"},{"location":"users/quickstart/#ambianic-ui-app","text":"Ambianic UI is a modern prorgressive web application (PWA) that provides Plug-and-Play pairing and remote access to Ambianic Edge devices. Read more about Ambianic UI.","title":"Ambianic UI app"},{"location":"users/quickstart/#development","text":"If you want to contribute to the code read how to setup a development environment","title":"Development"},{"location":"users/quickstart/#troubleshooting","text":"If you experience problems with your initial setup and you can't find a good solution online, feel free to engage the team on Twitter , Slack , or Github discussion forums .","title":"Troubleshooting"},{"location":"users/quickstart/#supporting-the-project","text":"If you find value in this project, consider supporting its future success. See the sustainability section.","title":"Supporting the project"},{"location":"users/standard-ai-apps/","text":"Standard AI Apps Included with the base Ambianic distribution Object Detection Face Detection Face Recognition Etc.","title":"Standard AI Apps Included with the base Ambianic distribution"},{"location":"users/standard-ai-apps/#standard-ai-apps-included-with-the-base-ambianic-distribution","text":"Object Detection Face Detection Face Recognition Etc.","title":"Standard AI Apps Included with the base Ambianic distribution"},{"location":"users/support/","text":"Support TBD","title":"Support"},{"location":"users/support/#support","text":"TBD","title":"Support"},{"location":"users/webui/","text":"Amianic Web UI Registration and login Securing access AI Apps Overview Pipelines Overview Advanced Pipeline Composition","title":"Amianic Web UI"},{"location":"users/webui/#amianic-web-ui","text":"Registration and login Securing access AI Apps Overview Pipelines Overview Advanced Pipeline Composition","title":"Amianic Web UI"}]}